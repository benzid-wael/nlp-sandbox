{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP for ML Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hypothesis**: Part of Speech (POS) tagging and syntactic dependency parsing provides valuable information for classifying imperative phrases. The thinking is that being able to detect imperative phrases will transfer well to detecting tasks and to-dos.\n",
    "\n",
    "#### Some Terminology\n",
    "- [_Imperative mood_](https://en.wikipedia.org/wiki/Imperative_mood) is \"used principally for ordering, requesting or advising the listener to do (or not to do) something... also often used for giving instructions as to how to perform a task.\"\n",
    "- _Part of speech (POS)_ is a way of categorizing a word based on its syntactic function.\n",
    "    - The POS tagger from Spacy.io that is used in this notebook differentiates between [*pos_* and *tag_*](https://spacy.io/docs/api/annotation#pos-tagging-english) - *POS (pos_)* refers to \"coarse-grained part-of-speech\" like `VERB`, `ADJ`, or `PUNCT`; and *POSTAG (tag_)* refers to \"fine-grained part-of-speech\" like `VB`, `JJ`, or `.`.\n",
    "- _Syntactic dependency parsing_ is a way of connecting words based on syntactic relationships, [such as](https://spacy.io/docs/api/annotation#dependency-parsing-english) `DOBJ` (direct object), `PREP` (prepositional modifier), or `POBJ` (object of preposition).\n",
    "    - Check out the dependency parse for the phrase [\"Send the report by Kyle by tomorrow\"](https://demos.explosion.ai/displacy/?text=Send%20the%20report%20by%20Kyle%20by%20tomorrow&model=en&cpu=1&cph=1) as an example\n",
    "\n",
    "#### Features\n",
    "The imperative mood centers around _actions_, and actions are generally represented in English using verbs. So the features are engineered to also center on the VERB:\n",
    "1. *FeatureName.VERB*: Does the phrase contain VERB(s) of the tag form VB*?\n",
    "2. *FeatureName.FOLLOWING_POS*: Are the words following the VERB(s) of certain parts of speech?\n",
    "3. *FeatureName.FOLLOWING_POSTAG*: Are the words following the VERB(s) of certain POS tags?\n",
    "4. *FeatureName.CHILD_DEP*: Are the VERB(s) parents of certain syntactic dependencies?\n",
    "5. *FeatureName.PARENT_DEP*: Are the VERB(s) children of certain syntactic dependencies?\n",
    "6. *FeatureName.CHILD_POS*: Are the syntactic dependencies that the VERB(s) are children of of certain parts of speech?\n",
    "7. *FeatureName.CHILD_POSTAG*: Are the syntactic dependencies that the VERB(s) are children of of certain POS tags?\n",
    "8. *FeatureName.PARENT_POS*: Are the syntactic dependencies that the VERB(s) parent of certain parts of speech?\n",
    "9. *FeatureName.PARENT_POSTAG*: Are the syntactic dependencies that the VERB(s) parent of certain POS tags?\n",
    "\n",
    "Note that features 2-9 all depend on feature 1 between `True`; if `False`, phrase vectorization will result in all zeroes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a recipe corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wrote and ran `epicurious_recipes.py`\\* to scrape Epicurious.com for recipe instructions and descriptions. Output is `epicurious-pos.txt` and `epicurious-neg.txt`.\n",
    "\n",
    "\\* _script loosely based off of https://github.com/benosment/hrecipe-parse_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that deriving all negative examples in the training set from Epicurious recipe descriptions would result in negative examples that are longer and syntactically more complicated than the positive examples. This is a form of bias.\n",
    "\n",
    "To (hopefully?) correct for this a bit, I will add the short movie reviews found at https://pythonprogramming.net/static/downloads/short_reviews/ as more negative examples.\n",
    "\n",
    "This still feels weird because we're selecting negative examples only from specific categories of text (recipe descriptions, short movie reviews) - just because they're readily available.\n",
    "\n",
    "Ultimately though, this recipe corpus is a **stopgap/proof of concept** for a corpus more relevant to tasks later on, so I won't worry further about this for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BASE_DIR = os.getcwd()\n",
    "pos_data_path = BASE_DIR + '/pos.txt'\n",
    "neg_data_path = BASE_DIR + '/neg.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(pos_data_path, 'r', encoding='utf-8') as f:\n",
    "    pos_data = f.read()\n",
    "with open(neg_data_path, 'r', encoding='utf-8') as f:\n",
    "    neg_data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_data_split = pos_data.split('\\n')\n",
    "neg_data_split = neg_data.split('\\n')\n",
    "\n",
    "num_pos = len(pos_data_split)\n",
    "num_neg = len(neg_data_split)\n",
    "\n",
    "# 50/50 split between the number of positive and negative samples\n",
    "num = num_pos if num_pos < num_neg else num_neg\n",
    "\n",
    "# shuffle samples\n",
    "random.shuffle(pos_data_split)\n",
    "random.shuffle(neg_data_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines = []\n",
    "for l in pos_data_split[:num]:\n",
    "    lines.append((l, 'pos'))\n",
    "for l in neg_data_split[:num]:\n",
    "    lines.append((l, 'neg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from enum import Enum, auto\n",
    "class FeatureName(Enum):\n",
    "    VERB = auto()\n",
    "    FOLLOWING_POS = auto()\n",
    "    FOLLOWING_POSTAG = auto()\n",
    "    CHILD_DEP = auto()\n",
    "    PARENT_DEP = auto()\n",
    "    CHILD_POS = auto()\n",
    "    CHILD_POSTAG = auto()\n",
    "    PARENT_POS = auto()\n",
    "    PARENT_POSTAG = auto()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [spaCy.io](https://spacy.io/) for NLP\n",
    "_Because Stanford CoreNLP is hard to install for Python_\n",
    "\n",
    "Found Spacy through an article on [\"Training a Classifier for Relation Extraction from Medical Literature\"](https://www.microsoft.com/developerblog/2016/09/13/training-a-classifier-for-relation-extraction-from-medical-literature/) ([GitHub](https://github.com/CatalystCode/corpus-to-graph-ml))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"nltk_library_comparison.png\" alt=\"NLTK library comparison chart https://spacy.io/docs/api/#comparison\" style=\"width: 400px; margin: 0;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!conda config --add channels conda-forge\n",
    "#!conda install spacy\n",
    "#!python -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Spacy Data Model for NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy's sentence segmentation is lacking... https://github.com/explosion/spaCy/issues/235. So each '\\n' will start a new Spacy Doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_spacy_docs(ll):\n",
    "    dd = [(nlp(l[0]), l[1]) for l in ll]\n",
    "    # collapse noun phrases into single compounds\n",
    "    for d in dd:\n",
    "        for np in d[0].noun_chunks:\n",
    "            np.merge(np.root.tag_, np.text, np.root.ent_type_)\n",
    "    return dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs = create_spacy_docs(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization, POS tagging, and dependency parsing happened automatically with the `nlp(line)` calls above! So let's look at the outputs.\n",
    "\n",
    "https://spacy.io/docs/usage/data-model and https://spacy.io/docs/api/doc will be useful going forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Whisk vigorously until the mixture comes together.]\n",
      "[Add shrimp and cook (no need to return to a boil), stirring gently, until shrimp turn pink, about 3 minutes.]\n",
      "[Beat the other egg in a small bowl and begin to stir it inâ€”you may not want to add all of it in the interest of dryness.]\n",
      "[Repeat with remaining steaks.]\n",
      "[Flip once, cover, and grill for another 4 to 6 minutes, until cooked through.]\n",
      "[Remove squid from bag, letting marinade drip off.]\n",
      "[Whisk sour cream and remaining 2 Tbsp., lime juice in a small bowl.]\n",
      "[Heat over medium-high until thermometer registers 350Â°F.]\n",
      "[Rub with 1 Tbsp. oil and 1/2 tsp. salt and transfer cut side down to another rimmed baking sheet.]\n",
      "[Transfer to a rimmed baking sheet and let cool.]\n"
     ]
    }
   ],
   "source": [
    "for doc in docs[:10]:\n",
    "    print(list(doc[0].sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Whisk, the mixture]\n",
      "[shrimp, cook, a boil, shrimp, pink]\n",
      "[the other egg, a small bowl, it, you, it, the interest, dryness]\n",
      "[remaining steaks]\n",
      "[Flip, grill, another 4 to 6 minutes]\n",
      "[squid, bag, marinade drip]\n",
      "[Whisk, cream, lime juice, a small bowl]\n",
      "[thermometer]\n",
      "[1 Tbsp, transfer, side, another rimmed baking sheet]\n",
      "[Transfer, a rimmed baking sheet, let cool]\n"
     ]
    }
   ],
   "source": [
    "for doc in docs[:10]:\n",
    "    print(list(doc[0].noun_chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Spacy's dependency graph visualization](https://demos.explosion.ai/displacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whisk ROOT Whisk PROPN NNP Whisk [comes, .]\n",
      "vigorously advmod vigorously ADV RB comes []\n",
      "until mark until ADP IN comes []\n",
      "the mixture nsubj the mixture NOUN NN comes []\n",
      "comes advcl come VERB VBZ Whisk [vigorously, until, the mixture, together]\n",
      "together advmod together ADV RB comes []\n",
      ". punct . PUNCT . Whisk []\n",
      "Add ROOT add VERB VB Add [shrimp, (, need, ,, stirring, .]\n",
      "shrimp dobj shrimp NOUN NN Add [and, cook]\n",
      "and cc and CCONJ CC shrimp []\n",
      "cook conj cook NOUN NN shrimp []\n",
      "( punct ( PUNCT -LRB- Add []\n",
      "no det no DET DT need []\n",
      "need npadvmod need NOUN NN Add [no, return, )]\n",
      "to aux to PART TO return []\n",
      "return acl return VERB VB need [to, to]\n",
      "to prep to ADP IN return [a boil]\n",
      "a boil pobj a boil NOUN NN to []\n",
      ") punct ) PUNCT -RRB- need []\n",
      ", punct , PUNCT , Add []\n",
      "stirring advcl stir VERB VBG Add [gently, ,, turn]\n",
      "gently advmod gently ADV RB stirring []\n",
      ", punct , PUNCT , stirring []\n",
      "until mark until ADP IN turn []\n",
      "shrimp nsubj shrimp NOUN NN turn []\n",
      "turn advcl turn VERB VBP stirring [until, shrimp, pink, minutes]\n",
      "pink dobj pink NOUN NN turn [,]\n",
      ", punct , PUNCT , pink []\n",
      "about advmod about ADP IN 3 []\n",
      "3 nummod 3 NUM CD minutes [about]\n",
      "minutes npadvmod minute NOUN NNS turn [3]\n",
      ". punct . PUNCT . Add []\n",
      "Beat nsubj beat VERB VB begin []\n",
      "the other egg nsubj the other egg NOUN NN begin [in, and]\n",
      "in prep in ADP IN the other egg [a small bowl]\n",
      "a small bowl pobj a small bowl NOUN NN in []\n",
      "and cc and CCONJ CC the other egg []\n",
      "begin ROOT begin VERB VB begin [Beat, the other egg, stir, want]\n",
      "to aux to PART TO stir []\n",
      "stir xcomp stir VERB VB begin [to, it, in, â€”]\n",
      "it dobj it PRON PRP stir []\n",
      "in prep in ADP IN stir []\n",
      "â€” punct â€” PUNCT NFP stir []\n",
      "you nsubj you PRON PRP want []\n",
      "may aux may VERB MD want []\n",
      "not neg not ADV RB want []\n",
      "want conj want VERB VB begin [you, may, not, add, .]\n",
      "to aux to PART TO add []\n",
      "add xcomp add VERB VB want [to, all, in]\n",
      "all dobj all DET DT add [of]\n",
      "of prep of ADP IN all [it]\n",
      "it pobj it PRON PRP of []\n",
      "in prep in ADP IN add [the interest]\n",
      "the interest pobj the interest NOUN NN in [of]\n",
      "of prep of ADP IN the interest [dryness]\n",
      "dryness pobj dryness NOUN NN of []\n",
      ". punct . PUNCT . want []\n",
      "Repeat ROOT repeat VERB VB Repeat [with, .]\n",
      "with prep with ADP IN Repeat [remaining steaks]\n",
      "remaining steaks pobj remaining steaks NOUN NNS with []\n",
      ". punct . PUNCT . Repeat []\n",
      "Flip nsubj Flip PROPN NNP cover [once]\n",
      "once advmod once ADV RB Flip [,]\n",
      ", punct , PUNCT , once []\n",
      "cover ROOT cover NOUN NN cover [Flip, ,, and, grill, cooked, .]\n",
      ", punct , PUNCT , cover []\n",
      "and cc and CCONJ CC cover []\n",
      "grill conj grill NOUN NN cover [for]\n",
      "for prep for ADP IN grill [another 4 to 6 minutes]\n",
      "another 4 to 6 minutes pobj another 4 to 6 minutes NOUN NNS for [,]\n",
      ", punct , PUNCT , another 4 to 6 minutes []\n",
      "until mark until ADP IN cooked []\n",
      "cooked advcl cook VERB VBN cover [until, through]\n",
      "through prt through PART RP cooked []\n",
      ". punct . PUNCT . cover []\n"
     ]
    }
   ],
   "source": [
    "for doc in docs[:5]:\n",
    "    for token in doc[0]:\n",
    "        print(token.text, token.dep_, token.lemma_, token.pos_, token.tag_, token.head, list(token.children))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Featurization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def featurize(d):\n",
    "    s_features = defaultdict(int)\n",
    "    for idx, token in enumerate(d):\n",
    "        #print(token, token.pos_, token.tag_)\n",
    "        if re.match(r'VB.?', token.tag_) is not None: # note: not using token.pos == VERB because this also includes BES, HVS, MD tags \n",
    "            s_features[FeatureName.VERB.name] += 1\n",
    "            # FOLLOWING_POS\n",
    "            next_idx = idx + 1;\n",
    "            if next_idx < len(d):\n",
    "                s_features[f'{FeatureName.FOLLOWING_POS.name}_{d[next_idx].pos_}'] += 1\n",
    "                s_features[f'{FeatureName.FOLLOWING_POSTAG.name}_{d[next_idx].tag_}'] += 1\n",
    "            # VERB_HEAD_DEP\n",
    "            # VERB_HEAD_POS\n",
    "            '''\n",
    "            \"Because the syntactic relations form a tree, every word has exactly one head.\n",
    "            You can therefore iterate over the arcs in the tree by iterating over the words in the sentence.\"\n",
    "            https://spacy.io/docs/usage/dependency-parse#navigating\n",
    "            '''\n",
    "            if (token.head is not token):\n",
    "                s_features[f'{FeatureName.PARENT_DEP.name}_{token.head.dep_.upper()}'] += 1\n",
    "                s_features[f'{FeatureName.PARENT_POS.name}_{token.head.pos_}'] += 1\n",
    "                s_features[f'{FeatureName.PARENT_POSTAG.name}_{token.head.tag_}'] += 1\n",
    "            # VERB_CHILD_DEP\n",
    "            # VERB_CHILD_POS\n",
    "            for child in token.children:\n",
    "                s_features[f'{FeatureName.CHILD_DEP.name}_{child.dep_.upper()}'] += 1\n",
    "                s_features[f'{FeatureName.CHILD_POS.name}_{child.pos_}'] += 1\n",
    "                s_features[f'{FeatureName.CHILD_POSTAG.name}_{child.tag_}'] += 1\n",
    "    return dict(s_features)\n",
    "        #print(dict(s_features))\n",
    "    #print()\n",
    "\n",
    "#print(featuresets, len(featuresets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featuresets = [(featurize(doc[0]), doc[1]) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats on feature set lengths:\n",
      "mean: 23.033783783783782\n",
      "stdev: 14.534981922236032\n",
      "median: 23.0\n",
      "mode: 0\n",
      "max: 75\n",
      "min: 0\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean, median, mode, stdev\n",
    "f_lengths = [len(fs[0]) for fs in featuresets]\n",
    "\n",
    "print('Stats on feature set lengths:')\n",
    "print(f'mean: {mean(f_lengths)}')\n",
    "print(f'stdev: {stdev(f_lengths)}')\n",
    "print(f'median: {median(f_lengths)}')\n",
    "print(f'mode: {mode(f_lengths)}')\n",
    "print(f'max: {max(f_lengths)}')\n",
    "print(f'min: {min(f_lengths)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'CHILD_DEP_ADVMOD': 2,\n",
       "   'CHILD_DEP_MARK': 1,\n",
       "   'CHILD_DEP_NSUBJ': 1,\n",
       "   'CHILD_POSTAG_IN': 1,\n",
       "   'CHILD_POSTAG_NN': 1,\n",
       "   'CHILD_POSTAG_RB': 2,\n",
       "   'CHILD_POS_ADP': 1,\n",
       "   'CHILD_POS_ADV': 2,\n",
       "   'CHILD_POS_NOUN': 1,\n",
       "   'FOLLOWING_POSTAG_RB': 1,\n",
       "   'FOLLOWING_POS_ADV': 1,\n",
       "   'PARENT_DEP_ROOT': 1,\n",
       "   'PARENT_POSTAG_NNP': 1,\n",
       "   'PARENT_POS_PROPN': 1,\n",
       "   'VERB': 1},\n",
       "  'pos'),\n",
       " ({'CHILD_DEP_ADVCL': 2,\n",
       "   'CHILD_DEP_ADVMOD': 1,\n",
       "   'CHILD_DEP_AUX': 1,\n",
       "   'CHILD_DEP_DOBJ': 2,\n",
       "   'CHILD_DEP_MARK': 1,\n",
       "   'CHILD_DEP_NPADVMOD': 2,\n",
       "   'CHILD_DEP_NSUBJ': 1,\n",
       "   'CHILD_DEP_PREP': 1,\n",
       "   'CHILD_DEP_PUNCT': 4,\n",
       "   'CHILD_POSTAG_,': 2,\n",
       "   'CHILD_POSTAG_-LRB-': 1,\n",
       "   'CHILD_POSTAG_.': 1,\n",
       "   'CHILD_POSTAG_IN': 2,\n",
       "   'CHILD_POSTAG_NN': 4,\n",
       "   'CHILD_POSTAG_NNS': 1,\n",
       "   'CHILD_POSTAG_RB': 1,\n",
       "   'CHILD_POSTAG_TO': 1,\n",
       "   'CHILD_POSTAG_VBG': 1,\n",
       "   'CHILD_POSTAG_VBP': 1,\n",
       "   'CHILD_POS_ADP': 2,\n",
       "   'CHILD_POS_ADV': 1,\n",
       "   'CHILD_POS_NOUN': 5,\n",
       "   'CHILD_POS_PART': 1,\n",
       "   'CHILD_POS_PUNCT': 4,\n",
       "   'CHILD_POS_VERB': 2,\n",
       "   'FOLLOWING_POSTAG_IN': 1,\n",
       "   'FOLLOWING_POSTAG_NN': 2,\n",
       "   'FOLLOWING_POSTAG_RB': 1,\n",
       "   'FOLLOWING_POS_ADP': 1,\n",
       "   'FOLLOWING_POS_ADV': 1,\n",
       "   'FOLLOWING_POS_NOUN': 2,\n",
       "   'PARENT_DEP_ADVCL': 1,\n",
       "   'PARENT_DEP_NPADVMOD': 1,\n",
       "   'PARENT_DEP_ROOT': 1,\n",
       "   'PARENT_POSTAG_NN': 1,\n",
       "   'PARENT_POSTAG_VB': 1,\n",
       "   'PARENT_POSTAG_VBG': 1,\n",
       "   'PARENT_POS_NOUN': 1,\n",
       "   'PARENT_POS_VERB': 2,\n",
       "   'VERB': 4},\n",
       "  'pos')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featuresets[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.shuffle(featuresets)\n",
    "\n",
    "split_num = round(num / 5)\n",
    "\n",
    "# train and test sets\n",
    "testing_set = featuresets[:split_num]\n",
    "training_set =  featuresets[split_num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# decoupling the functionality of nltk.classify.accuracy\n",
    "def predict(classifier, gold):\n",
    "    predictions = classifier.classify_many([fs for (fs, l) in gold])\n",
    "    return list(zip([l for (fs, l) in gold], predictions))\n",
    "\n",
    "def accuracy(predict):\n",
    "    correct = [label == prediction for (label, prediction) in predict]\n",
    "    if correct:\n",
    "        return sum(correct) / len(correct)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaiveBayes classifier accuracy percent: 64.4880174291939\n",
      "MultinomialNB classifier accuracy percent: 76.25272331154684\n",
      "BernoulliNB classifier accuracy percent: 76.03485838779956\n",
      "LogisticRegression classifier accuracy percent: 83.87799564270153\n",
      "SGDClassifier classifier accuracy percent: 77.99564270152506\n",
      "SVC classifier accuracy percent: 83.66013071895425\n",
      "LinearSVC classifier accuracy percent: 83.87799564270153\n",
      "DecisionTree classifier accuracy percent: 79.520697167756\n"
     ]
    }
   ],
   "source": [
    "from nltk import NaiveBayesClassifier\n",
    "from nltk.classify.decisiontree import DecisionTreeClassifier\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "\n",
    "nb = NaiveBayesClassifier.train(training_set)\n",
    "nb_predict = predict(nb, testing_set)\n",
    "nb_accuracy = accuracy(nb_predict)\n",
    "print(\"NaiveBayes classifier accuracy percent:\", nb_accuracy*100)\n",
    "\n",
    "multinomial_nb = SklearnClassifier(MultinomialNB())\n",
    "multinomial_nb.train(training_set)\n",
    "mnb_predict = predict(multinomial_nb, testing_set)\n",
    "mnb_accuracy = accuracy(mnb_predict)\n",
    "print(\"MultinomialNB classifier accuracy percent:\", mnb_accuracy*100)\n",
    "\n",
    "bernoulli_nb = SklearnClassifier(BernoulliNB())\n",
    "bernoulli_nb.train(training_set)\n",
    "bnb_predict = predict(bernoulli_nb, testing_set)\n",
    "bnb_accuracy = accuracy(bnb_predict)\n",
    "print(\"BernoulliNB classifier accuracy percent:\", bnb_accuracy*100)\n",
    "\n",
    "logistic_regression = SklearnClassifier(LogisticRegression())\n",
    "logistic_regression.train(training_set)\n",
    "lr_predict = predict(logistic_regression, testing_set)\n",
    "lr_accuracy = accuracy(lr_predict)\n",
    "print(\"LogisticRegression classifier accuracy percent:\", lr_accuracy*100)\n",
    "\n",
    "sgd = SklearnClassifier(SGDClassifier())\n",
    "sgd.train(training_set)\n",
    "sgd_predict = predict(sgd, testing_set)\n",
    "sgd_accuracy = accuracy(sgd_predict)\n",
    "print(\"SGDClassifier classifier accuracy percent:\", sgd_accuracy*100)\n",
    "\n",
    "svc = SklearnClassifier(SVC())\n",
    "svc.train(training_set)\n",
    "svc_predict = predict(svc, testing_set)\n",
    "svc_accuracy = accuracy(svc_predict)\n",
    "print(\"SVC classifier accuracy percent:\", svc_accuracy*100)\n",
    "\n",
    "linear_svc = SklearnClassifier(LinearSVC())\n",
    "linear_svc.train(training_set)\n",
    "linear_svc_predict = predict(linear_svc, testing_set)\n",
    "linear_svc_accuracy = accuracy(linear_svc_predict)\n",
    "print(\"LinearSVC classifier accuracy percent:\", linear_svc_accuracy*100)\n",
    "\n",
    "# slow\n",
    "dt = DecisionTreeClassifier.train(training_set)\n",
    "dt_predict = predict(dt, testing_set)\n",
    "dt_accuracy = accuracy(dt_predict)\n",
    "print(\"DecisionTree classifier accuracy percent:\", dt_accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC: ['pos', 'pos', 'pos', 'pos', 'pos', 'pos', 'pos']\n",
      "LogisticRegression: ['pos', 'pos', 'pos', 'pos', 'pos', 'pos', 'pos']\n",
      "SGD: ['pos', 'pos', 'pos', 'pos', 'pos', 'pos', 'pos']\n"
     ]
    }
   ],
   "source": [
    "phrases = [\"Mow lawn\", \"Mow the lawn\", \"Buy new shoes\", \"Feed the dog\", \"Send report to Kyle\", \"Send the report to Kyle\", \"Peel the potatoes\"]\n",
    "features = [featurize(nlp(phrase)) for phrase in phrases]\n",
    "\n",
    "predict_linear_svc = linear_svc.classify_many(features)\n",
    "predict_logistic = logistic_regression.classify_many(features)\n",
    "predict_sgd = sgd.classify_many(features)\n",
    "\n",
    "print(f'LinearSVC: {predict_linear_svc}')\n",
    "print(f'LogisticRegression: {predict_logistic}')\n",
    "print(f'SGD: {predict_sgd}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the highest performing classifiers (`LogisticRegression, LinearSVC: 88.89`) are producing quite different results on our sample task list:\n",
    "    - \"Mow lawn\"\n",
    "        - LinearSVC: neg\n",
    "        - LogisticRegression: neg\n",
    "    - \"Mow the lawn\"\n",
    "        - LinearSVC: pos\n",
    "        - LogisticRegression: neg\n",
    "    - \"Buy new shoes\"\n",
    "        - LinearSVC: pos\n",
    "        - LogisticRegression: pos\n",
    "    - \"Feed the dog\"\n",
    "        - LinearSVC: pos\n",
    "        - LogisticRegression: neg\n",
    "    - \"Send report to Kyle\"\n",
    "        - LinearSVC: neg\n",
    "        - LogisticRegression: neg\n",
    "    - \"Send the report to Kyle\"\n",
    "        - LinearSVC: pos\n",
    "        - LogisticRegression: neg\n",
    "\n",
    "Observations:\n",
    "1. LogisticRegression seems _heavily_ biased to be negative.\n",
    "2. LinearSVC seems more fragile when grammar is off (e.g., missing _the_'s) - however, this feels fixable with a more varied/realistic training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression classifier accuracy percent: 83.87799564270153\n",
      "SGDClassifier classifier accuracy percent: 73.20261437908496\n",
      "LinearSVC classifier accuracy percent: 83.87799564270153\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(training_set)\n",
    "\n",
    "logistic_regression.train(training_set)\n",
    "lr_predict = predict(logistic_regression, testing_set)\n",
    "lr_accuracy = accuracy(lr_predict)\n",
    "print(\"LogisticRegression classifier accuracy percent:\", lr_accuracy*100)\n",
    "\n",
    "sgd.train(training_set)\n",
    "sgd_predict = predict(sgd, testing_set)\n",
    "sgd_accuracy = accuracy(sgd_predict)\n",
    "print(\"SGDClassifier classifier accuracy percent:\", sgd_accuracy*100)\n",
    "\n",
    "linear_svc.train(training_set)\n",
    "linear_svc_predict = predict(linear_svc, testing_set)\n",
    "linear_svc_accuracy = accuracy(linear_svc_predict)\n",
    "print(\"LinearSVC classifier accuracy percent:\", linear_svc_accuracy*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`LogisticRegression_classifier` and `LinearSVC_classifier` accuracies did not change with another epoch on randomly shuffled training data. `SGDClassifier_classifier` however did (as I suspected it might from my deep learning course).\n",
    "\n",
    "So let's run more epochs with `SGDClassifier_classifier` for now (until I learn if multiple epochs can work with other types of classifiers)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier classifier accuracy percent (epoch 1): 77.99564270152506\n",
      "SGDClassifier classifier accuracy percent (epoch 2): 84.31372549019608\n",
      "SGDClassifier classifier accuracy percent (epoch 3): 81.48148148148148\n",
      "SGDClassifier classifier accuracy percent (epoch 4): 82.78867102396515\n",
      "SGDClassifier classifier accuracy percent (epoch 5): 77.77777777777779\n",
      "SGDClassifier classifier accuracy percent (epoch 6): 78.21350762527233\n",
      "SGDClassifier classifier accuracy percent (epoch 7): 82.13507625272331\n",
      "SGDClassifier classifier accuracy percent (epoch 8): 82.13507625272331\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 8\n",
    "for i in range(num_epochs):\n",
    "    random.shuffle(training_set)\n",
    "\n",
    "    sgd.train(training_set)\n",
    "    sgd_predict = predict(sgd, testing_set)\n",
    "    sgd_accuracy = accuracy(sgd_predict)\n",
    "    print(f\"SGDClassifier classifier accuracy percent (epoch {i+1}):\", sgd_accuracy*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most Informative Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/a/11140887\n",
    "def show_most_informative_features(vectorizer, clf, n=20):\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))\n",
    "    top = zip(coefs_with_fns[:n], coefs_with_fns[:-(n + 1):-1])\n",
    "    for (coef_1, fn_1), (coef_2, fn_2) in top:\n",
    "        print(\"\\t%.4f\\t%-15s\\t\\t%.4f\\t%-15s\" % (coef_1, fn_1, coef_2, fn_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regressions's\n",
      "Most Informative Features\n",
      "\t-2.3360\tCHILD_DEP_AGENT\t\t2.0531\tCHILD_POSTAG_-RRB-\n",
      "\t-1.9459\tCHILD_POSTAG_HYPH\t\t1.4447\tCHILD_DEP_NPADVMOD\n",
      "\t-1.7239\tCHILD_POSTAG_``\t\t1.3925\tFOLLOWING_POS_NUM\n",
      "\t-1.5496\tCHILD_DEP_NSUBJ\t\t1.3925\tFOLLOWING_POSTAG_CD\n",
      "\t-1.4814\tCHILD_DEP_INTJ \t\t1.2902\tFOLLOWING_POSTAG_RB\n",
      "\t-1.4610\tPARENT_DEP_AMOD\t\t1.2464\tCHILD_DEP_DOBJ||XCOMP\n",
      "\t-1.3615\tPARENT_POSTAG_VBZ\t\t1.2198\tCHILD_POSTAG_WRB\n",
      "\t-1.3491\tFOLLOWING_POSTAG_VB\t\t1.2187\tVERB           \n",
      "\t-1.3442\tFOLLOWING_POSTAG_WRB\t\t1.2039\tCHILD_POSTAG_NFP\n",
      "\t-1.2937\tFOLLOWING_POSTAG_-RRB-\t\t1.1131\tPARENT_POS_PROPN\n",
      "\t-1.2319\tCHILD_DEP_ATTR \t\t1.1131\tPARENT_POSTAG_NNP\n",
      "\t-1.2314\tCHILD_DEP_NEG  \t\t1.0512\tCHILD_POSTAG_MD\n",
      "\t-1.2281\tCHILD_POSTAG_WP\t\t1.0500\tFOLLOWING_POSTAG_JJ\n",
      "\t-1.1658\tPARENT_DEP_DATIVE\t\t1.0041\tCHILD_POS_PROPN\n",
      "\t-1.1486\tCHILD_DEP_AUX  \t\t0.9206\tPARENT_DEP_DEP \n",
      "LinearSVC's\n",
      "Most Informative Features\n",
      "\t-1.5210\tCHILD_POSTAG_``\t\t1.4167\tCHILD_DEP_ADVMOD||XCOMP\n",
      "\t-1.1372\tCHILD_DEP_AGENT\t\t1.3514\tFOLLOWING_POSTAG_JJS\n",
      "\t-1.0790\tFOLLOWING_POSTAG_-RRB-\t\t1.1541\tPARENT_DEP_ADVMOD||CONJ\n",
      "\t-0.9951\tFOLLOWING_POSTAG_WRB\t\t1.0858\tCHILD_DEP_DOBJ||XCOMP\n",
      "\t-0.9815\tCHILD_DEP_INTJ \t\t1.0671\tCHILD_POSTAG_-RRB-\n",
      "\t-0.9687\tPARENT_DEP_AMOD\t\t0.9360\tCHILD_POSTAG_NFP\n",
      "\t-0.9154\tFOLLOWING_POSTAG_PRP$\t\t0.8893\tCHILD_DEP_PUNCT\n",
      "\t-0.8950\tCHILD_DEP_PRECONJ\t\t0.8667\tCHILD_DEP_META \n",
      "\t-0.8712\tCHILD_POS_PUNCT\t\t0.8383\tCHILD_DEP_POBJ \n",
      "\t-0.8421\tCHILD_POSTAG_HYPH\t\t0.8056\tFOLLOWING_POSTAG_NFP\n",
      "\t-0.7551\tPARENT_POSTAG_PRP\t\t0.7163\tVERB           \n",
      "\t-0.7551\tPARENT_POS_PRON\t\t0.6908\tPARENT_DEP_DOBJ||XCOMP\n",
      "\t-0.7551\tPARENT_DEP_DATIVE\t\t0.6833\tCHILD_DEP_POBJ||PREP\n",
      "\t-0.6553\tFOLLOWING_POSTAG_JJR\t\t0.6446\tCHILD_POSTAG_WRB\n",
      "\t-0.6501\tFOLLOWING_POS_ADV\t\t0.6392\tFOLLOWING_POSTAG_RB\n"
     ]
    }
   ],
   "source": [
    "#print(\"Naive Bayes'\")\n",
    "#classifier.show_most_informative_features(15)\n",
    "\n",
    "print(\"Logistic Regressions's\")\n",
    "print('Most Informative Features')\n",
    "show_most_informative_features(logistic_regression._vectorizer, logistic_regression._clf, 15)\n",
    "\n",
    "print('LinearSVC\\'s')\n",
    "print('Most Informative Features')\n",
    "show_most_informative_features(linear_svc._vectorizer, linear_svc._clf, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scikit Learn metrics: Confusion matrix, Classification report, F1 score, Log loss\n",
    "\n",
    "http://scikit-learn.org/stable/modules/model_evaluation.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** log loss requires `predict_proba`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "    \n",
    "# http://scikit-learn.org/stable/modules/model_evaluation.html#precision-recall-f-measure-metrics\n",
    "def f1_macro(predict):\n",
    "    labels, predictions = zip(*predict)\n",
    "    return metrics.f1_score(labels, predictions, average='macro')\n",
    "\n",
    "def classification_report(predict):\n",
    "    labels, predictions = zip(*predict)\n",
    "    return metrics.classification_report(labels, predictions)\n",
    "\n",
    "def confusion_matrix(predict):\n",
    "    labels, predictions = zip(*predict)\n",
    "    print('layout:\\n[[tn   fp]\\n [fn   tp]]\\n')\n",
    "    return metrics.confusion_matrix(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6095900061052929"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_macro(nb_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.58      0.96      0.73       225\n",
      "        pos       0.91      0.34      0.49       234\n",
      "\n",
      "avg / total       0.75      0.64      0.61       459\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(nb_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layout:\n",
      "[[tn   fp]\n",
      " [fn   tp]]\n",
      "\n",
      "[[217   8]\n",
      " [155  79]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(nb_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#log_loss(nb_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next up**: digging into the results (confusion matrix, most informative features), comparing results to LUIS model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps and Improvements\n",
    "\n",
    "1. Training set may be too specific/not relevant enough (recipe instructions for positive dataset, recipe descriptions+short movie reviews for negative dataset)\n",
    "2. Throwing features into a blender - need to understand value of each\n",
    "3. Need to review different classifiers, strengths/weaknesses\n",
    "4. Phrase vectorizations of all 0s\n",
    "5. Varying feature vector lengths\n",
    "6. Voting\n",
    "7. Reducing dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things abandoned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I needed a library that supports dependency parsing, which NLTK does not... so I thought I'd add the [Stanford CoreNLP](https://stanfordnlp.github.io/CoreNLP/) toolkit and [its associated software](https://nlp.stanford.edu/software/) to NLTK. However, there are many conflicting instructions for installing the Java-based project, depending on NLTK version used. By the time I figured this out, the installation had become a time sink. So I abandoned this effort in favor of Spacy.io.\n",
    "\n",
    "I might return this way if I want to improve results/implement a voter system between the various linguistic and classification methods later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = [s for l in lines for s in sent_tokenize(l)] # punkt\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagged_sentences = []\n",
    "for s in sentences:\n",
    "    words = word_tokenize(s)\n",
    "    tagged = nltk.pos_tag(words) # averaged_perceptron_tagger\n",
    "    tagged_sentences.append(tagged)\n",
    "print(tagged_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: POS accuracy\n",
    "\n",
    "`Run down to the shop, will you, Peter` is parsed unexpectedly by `nltk.pos_tag`:\n",
    "> `[('Run', 'NNP'), ('down', 'RB'), ('to', 'TO'), ('the', 'DT'), ('shop', 'NN'), (',', ','), ('will', 'MD'), ('you', 'PRP'), (',', ','), ('Peter', 'NNP')]`\n",
    "\n",
    "`Run` is tagged as a `NNP (proper noun, singular)`\n",
    "\n",
    "I expected an output more like what the [Stanford Parser](http://nlp.stanford.edu:8080/parser/) provides:\n",
    "> `Run/VBG down/RP to/TO the/DT shop/NN ,/, will/MD you/PRP ,/, Peter/NNP`\n",
    "\n",
    "`Run` is tagged as a `VGB (verb, gerund/present participle)` - still not quite the `VB` I want, but at least it's a `V*`\n",
    "\n",
    "_MEANWHILE..._\n",
    "\n",
    "`nltk.pos_tag` did better with:\n",
    "> `[('Do', 'VB'), ('not', 'RB'), ('clean', 'VB'), ('soot', 'NN'), ('off', 'IN'), ('the', 'DT'), ('window', 'NN')]`\n",
    "\n",
    "Compared to [Stanford CoreNLP](http://nlp.stanford.edu:8080/corenlp/process) (note that this is different than what [Stanford Parser](http://nlp.stanford.edu:8080/parser/) outputs):\n",
    "> `(ROOT (S (VP (VB Do) (NP (RB not) (JJ clean) (NN soot)) (PP (IN off) (NP (DT the) (NN window))))))`\n",
    "\n",
    "Concern: _clean_ as `VB (verb, base form)` vs `JJ (adjective)` \n",
    "\n",
    "**IMPROVE** POS taggers should vote: nltk.pos_tag (averaged_perceptron_tagger), Stanford Parser, CoreNLP, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note what Spacy POS tagger did with `Run down to the shop, will you Peter`:\n",
    "\n",
    "`Run/VB down/RP to/IN the shop/NN ,/, will/MD you/PRP ,/, Peter/NNP`\n",
    "\n",
    "    where `Run` is the `VB` I expected from POS tagging (compared to `nltk.pos_tag` result of `NNP`). Also note that Spacy collapses `the shop` into a single unit, which should be helpful during featurization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Featurization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "featuresets = []\n",
    "for ts in tagged_sentences:\n",
    "    s_features = defaultdict(int)\n",
    "    for idx, tup in enumerate(ts):\n",
    "        #print(tup)\n",
    "        pos = tup[1]\n",
    "        # FeatureName.VERB\n",
    "        is_verb = re.match(r'VB.?', pos) is not None\n",
    "        print(tup, is_verb)\n",
    "        if is_verb:\n",
    "            s_features[FeatureName.VERB] += 1\n",
    "            # FOLLOWING_POS\n",
    "            next_idx = idx + 1;\n",
    "            if next_idx < len(ts):\n",
    "                s_features[f'{FeatureName.FOLLOWING}_{ts[next_idx][1]}'] += 1\n",
    "            # VERB_MODIFIER\n",
    "            # VERB_MODIFYING\n",
    "        else:\n",
    "            s_features[FeatureName.VERB] = 0\n",
    "    featuresets.append(dict(s_features))\n",
    "\n",
    "print()\n",
    "print(featuresets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Stanford NLP](https://nlp.stanford.edu/software/)\n",
    "Setup guide used: https://stackoverflow.com/a/34112695"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get dependency parser, NER, POS tagger\n",
    "!wget https://nlp.stanford.edu/software/stanford-parser-full-2017-06-09.zip\n",
    "!wget https://nlp.stanford.edu/software/stanford-ner-2017-06-09.zip\n",
    "!wget https://nlp.stanford.edu/software/stanford-postagger-full-2017-06-09.zip\n",
    "!unzip stanford-parser-full-2017-06-09.zip\n",
    "!unzip stanford-ner-2017-06-09.zip\n",
    "!unzip stanford-postagger-full-2017-06-09.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.parse.stanford import StanfordParser\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "from nltk.parse.stanford import StanfordNeuralDependencyParser\n",
    "from nltk.tag.stanford import StanfordPOSTagger, StanfordNERTagger\n",
    "from nltk.tokenize.stanford import StanfordTokenizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
