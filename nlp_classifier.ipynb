{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BASE_DIR = os.getcwd()\n",
    "pos_data_path = BASE_DIR + '/pos.txt'\n",
    "neg_data_path = BASE_DIR + '/neg.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pos_data_path, 'r', encoding='utf-8') as f:\n",
    "    pos_data = f.read()\n",
    "with open(neg_data_path, 'r', encoding='utf-8') as f:\n",
    "    neg_data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_data_split = pos_data.split('\\n')\n",
    "neg_data_split = neg_data.split('\\n')\n",
    "\n",
    "num_pos = len(pos_data_split)\n",
    "num_neg = len(neg_data_split)\n",
    "# 50/50 split between the number of positive and negative samples\n",
    "num = num_pos if num_pos > num_neg else num_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines = []\n",
    "for l in pos_data_split[:num]:\n",
    "    lines.append((l, 'pos'))\n",
    "for l in neg_data_split[:num]:\n",
    "    lines.append((l, 'neg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from enum import Enum, auto\n",
    "class FeatureName(Enum):\n",
    "    VERB = auto() # does this sentence contain a VB*?\n",
    "    FOLLOWING = auto() # is the following word a <POS>? postfixed with _<POS>\n",
    "    VERB_CHILD_DEP = auto() # what are the child (outgoing edges) dependencies (arc labels)? postfixed with _<DEP>\n",
    "    VERB_HEAD_DEP = auto() # what are the head (incoming edge) dependencies (arc labels)? postfixed with _<DEP>\n",
    "    VERB_CHILD_POS = auto() # is the child dependency a <POS>? postfixed with _<POS>\n",
    "    VERB_HEAD_POS = auto() # is the head dependency a <POS>? postfixed with _<POS>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [spaCy.io](https://spacy.io/)\n",
    "_Because Stanford CoreNLP is hard to install for Python_\n",
    "\n",
    "Found Spacy through an article on [\"Training a Classifier for Relation Extraction from Medical Literature\"](https://www.microsoft.com/developerblog/2016/09/13/training-a-classifier-for-relation-extraction-from-medical-literature/) ([GitHub](https://github.com/CatalystCode/corpus-to-graph-ml))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"nltk_library_comparison.png\" alt=\"NLTK library comparison chart https://spacy.io/docs/api/#comparison\" style=\"width: 400px; margin: 0;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!conda config --add channels conda-forge\n",
    "!conda install spacy\n",
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Spacy Data Model for NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy's sentence segmentation is lacking... https://github.com/explosion/spaCy/issues/235. So each '\\n' will start a new Spacy Doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spacy_docs(ll):\n",
    "    dd = [(nlp(l[0]), l[1]) for l in ll]\n",
    "    # collapse noun phrases into single compounds\n",
    "    for d in dd:\n",
    "        for np in d[0].noun_chunks:\n",
    "            np.merge(np.root.tag_, np.text, np.root.ent_type_)\n",
    "    return dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = create_spacy_docs(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization, POS tagging, and syntactic parsing happened automatically with the `nlp(line)` calls above! So let's look at these outputs.\n",
    "\n",
    "https://spacy.io/docs/usage/data-model and https://spacy.io/docs/api/doc will be useful going forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Be kind]\n",
      "[Get out of here]\n",
      "[Look this over]\n",
      "[Paul, do your homework now]\n",
      "[Do not clean soot off the window]\n",
      "[Turn your phones off, please]\n",
      "[Run down to the shop, will you, Peter]\n",
      "[Look at this]\n",
      "[Stir until smooth]\n",
      "[Pick up milk]\n"
     ]
    }
   ],
   "source": [
    "for doc in docs[:10]:\n",
    "    print(list(doc[0].sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n",
      "[Paul, your homework]\n",
      "[soot, the window]\n",
      "[your phones]\n",
      "[the shop, you]\n",
      "[]\n",
      "[]\n",
      "[milk]\n"
     ]
    }
   ],
   "source": [
    "for doc in docs[:10]:\n",
    "    print(list(doc[0].noun_chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Spacy's dependency graph visualization](https://demos.explosion.ai/displacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Be ROOT be VERB VB Be [kind]\n",
      "kind acomp kind ADJ JJ Be []\n",
      "Get ROOT get VERB VB Get [out]\n",
      "out prep out ADP IN Get [of]\n",
      "of prep of ADP IN out [here]\n",
      "here pcomp here ADV RB of []\n",
      "Look ROOT look VERB VB Look [this, over]\n",
      "this dobj this DET DT Look []\n",
      "over prep over ADP IN Look []\n",
      "Paul nsubj Paul PROPN NNP do [,]\n",
      ", punct , PUNCT , Paul []\n",
      "do ROOT do VERB VB do [Paul, your homework, now]\n",
      "your homework dobj your homework NOUN NN do []\n",
      "now advmod now ADV RB do []\n",
      "Do ROOT do VERB VBP Do [clean]\n",
      "not neg not ADV RB clean []\n",
      "clean acomp clean ADJ JJ Do [not, soot]\n",
      "soot dobj soot NOUN NN clean [off]\n",
      "off prep off ADP IN soot [the window]\n",
      "the window pobj the window NOUN NN off []\n",
      "Turn ROOT turn VERB VB Turn [your phones, off, ,, please]\n",
      "your phones dobj your phones NOUN NNS Turn []\n",
      "off prt off PART RP Turn []\n",
      ", punct , PUNCT , Turn []\n",
      "please intj please INTJ UH Turn []\n",
      "Run ROOT run VERB VB Run [down, to, ,, will]\n",
      "down prt down PART RP Run []\n",
      "to prep to ADP IN Run [the shop]\n",
      "the shop pobj the shop NOUN NN to []\n",
      ", punct , PUNCT , Run []\n",
      "will conj will VERB MD Run [you]\n",
      "you nsubj you PRON PRP will [,, Peter]\n",
      ", punct , PUNCT , you []\n",
      "Peter appos peter PROPN NNP you []\n",
      "Look ROOT look VERB VB Look [at]\n",
      "at prep at ADP IN Look [this]\n",
      "this pobj this DET DT at []\n",
      "Stir ROOT stir VERB VB Stir [until]\n",
      "until prep until ADP IN Stir [smooth]\n",
      "smooth pobj smooth ADJ JJ until []\n",
      "Pick ROOT pick VERB VB Pick [up, milk]\n",
      "up prt up PART RP Pick []\n",
      "milk dobj milk NOUN NN Pick []\n"
     ]
    }
   ],
   "source": [
    "for doc in docs[:10]:\n",
    "    for token in doc[0]:\n",
    "        print(token.text, token.dep_, token.lemma_, token.pos_, token.tag_, token.head, list(token.children))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note what Spacy POS tagger did with `Run down to the shop, will you Peter`:\n",
    "\n",
    "`Run/VB down/RP to/IN the shop/NN ,/, will/MD you/PRP ,/, Peter/NNP`\n",
    "\n",
    "where `Run` is the `VB` I expected earlier from POS tagging. Also note that `the shop` has been collapsed to a single compound, which will be helpful during featurization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Featurization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def featurize(d):\n",
    "    fs = []\n",
    "    s_features = defaultdict(int)\n",
    "    for idx, token in enumerate(d):\n",
    "        #print(token, token.pos_, token.tag_)\n",
    "        if re.match(r'VB.?', token.tag_) is not None: # note: not using token.pos == VERB because this also includes BES, HVS, MD tags \n",
    "            s_features[FeatureName.VERB.name] += 1\n",
    "            # FOLLOWING_POS\n",
    "            next_idx = idx + 1;\n",
    "            if next_idx < len(d):\n",
    "                s_features[f'{FeatureName.FOLLOWING.name}_{d[next_idx].tag_}'] += 1\n",
    "            # VERB_HEAD_DEP\n",
    "            # VERB_HEAD_POS\n",
    "            '''\n",
    "            \"Because the syntactic relations form a tree, every word has exactly one head.\n",
    "            You can therefore iterate over the arcs in the tree by iterating over the words in the sentence.\"\n",
    "            https://spacy.io/docs/usage/dependency-parse#navigating\n",
    "            '''\n",
    "            if (token.head is not token):\n",
    "                s_features[f'{FeatureName.VERB_HEAD_DEP.name}_{token.head.dep_.upper()}'] += 1\n",
    "                s_features[f'{FeatureName.VERB_HEAD_POS.name}_{token.head.tag_}'] += 1\n",
    "            # VERB_CHILD_DEP\n",
    "            # VERB_CHILD_POS\n",
    "            for child in token.children:\n",
    "                s_features[f'{FeatureName.VERB_CHILD_DEP.name}_{child.dep_.upper()}'] += 1\n",
    "                s_features[f'{FeatureName.VERB_CHILD_POS.name}_{child.tag_}'] += 1            \n",
    "    return dict(s_features)\n",
    "        #print(dict(s_features))\n",
    "    #print()\n",
    "\n",
    "#print(featuresets, len(featuresets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(featurize(doc[0]), doc[1]) for doc in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a recipe corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wrote and ran `epicurious_recipes.py`\\* to scrape Epicurious.com for recipe instructions and descriptions. Output is `epicurious-pos.txt` and `epicurious-neg.txt`.\n",
    "\n",
    "\\* _script loosely based off of https://github.com/benosment/hrecipe-parse_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that building a training set entirely from recipe descriptions would result in negative examples that are longer and syntactically more complicated than the positive examples. This is a form of bias.\n",
    "\n",
    "To (hopefully?) correct for this a bit, I will combine the short movie reviews at https://pythonprogramming.net/static/downloads/short_reviews/ as more negative examples.\n",
    "\n",
    "Ultimately though, this recipe corpus is a stopgap for more relevant corpus later on, so I won't worry further about this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(featuresets)\n",
    "\n",
    "split_num = round(num / 5)\n",
    "\n",
    "# train and test sets\n",
    "training_set = featuresets[:split_num]\n",
    "testing_set =  featuresets[split_num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Algo accuracy percent: 84.00358262427228\n",
      "MNB_classifier accuracy percent: 87.0577698163905\n",
      "BernoulliNB_classifier accuracy percent: 77.25929243170623\n",
      "LogisticRegression_classifier accuracy percent: 88.14151365875503\n",
      "SGDClassifier_classifier accuracy percent: 87.8280340349306\n",
      "LinearSVC_classifier accuracy percent: 88.40125391849529\n"
     ]
    }
   ],
   "source": [
    "from nltk import classify, NaiveBayesClassifier\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "\n",
    "classifier = NaiveBayesClassifier.train(training_set)\n",
    "print(\"Naive Bayes Algo accuracy percent:\", (classify.accuracy(classifier, testing_set))*100)\n",
    "\n",
    "MNB_classifier = SklearnClassifier(MultinomialNB())\n",
    "MNB_classifier.train(training_set)\n",
    "print(\"MNB_classifier accuracy percent:\", (classify.accuracy(MNB_classifier, testing_set))*100)\n",
    "\n",
    "BernoulliNB_classifier = SklearnClassifier(BernoulliNB())\n",
    "BernoulliNB_classifier.train(training_set)\n",
    "print(\"BernoulliNB_classifier accuracy percent:\", (classify.accuracy(BernoulliNB_classifier, testing_set))*100)\n",
    "\n",
    "LogisticRegression_classifier = SklearnClassifier(LogisticRegression())\n",
    "LogisticRegression_classifier.train(training_set)\n",
    "print(\"LogisticRegression_classifier accuracy percent:\", (classify.accuracy(LogisticRegression_classifier, testing_set))*100)\n",
    "\n",
    "SGDClassifier_classifier = SklearnClassifier(SGDClassifier())\n",
    "SGDClassifier_classifier.train(training_set)\n",
    "print(\"SGDClassifier_classifier accuracy percent:\", (classify.accuracy(SGDClassifier_classifier, testing_set))*100)\n",
    "\n",
    "##SVC_classifier = SklearnClassifier(SVC())\n",
    "##SVC_classifier.train(training_set)\n",
    "##print(\"SVC_classifier accuracy percent:\", (nltk.classify.accuracy(SVC_classifier, testing_set))*100)\n",
    "\n",
    "LinearSVC_classifier = SklearnClassifier(LinearSVC())\n",
    "LinearSVC_classifier.train(training_set)\n",
    "print(\"LinearSVC_classifier accuracy percent:\", (classify.accuracy(LinearSVC_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC: pos\n",
      "NaiveBayes: neg\n"
     ]
    }
   ],
   "source": [
    "phrase = \"Pick up milk\"\n",
    "feature = featurize(nlp(phrase))\n",
    "\n",
    "predict_linearSVC = LinearSVC_classifier.classify_many(feature)[0]\n",
    "predict_naivebayes = classifier.classify_many([feature])[0]\n",
    "\n",
    "print(f'LinearSVC: {predict_linearSVC}')\n",
    "print(f'NaiveBayes: {predict_naivebayes}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next up**: digging into the results (confusion matrix), improving results, comparing results to LUIS model, reducing dimensionality, VoteClassifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes'\n",
      "Most Informative Features\n",
      "      VERB_CHILD_POS_WDT = 1                 neg : pos    =     36.8 : 1.0\n",
      "       VERB_HEAD_POS_NNP = 1                 pos : neg    =     34.2 : 1.0\n",
      "       VERB_CHILD_POS_WP = 1                 neg : pos    =     18.4 : 1.0\n",
      "       VERB_HEAD_POS_VBZ = 1                 neg : pos    =     11.8 : 1.0\n",
      "      VERB_CHILD_POS_PRP = 3                 neg : pos    =      8.7 : 1.0\n",
      "       VERB_HEAD_POS_VBZ = 2                 neg : pos    =      8.3 : 1.0\n",
      "      VERB_HEAD_DEP_NMOD = 1                 pos : neg    =      8.1 : 1.0\n",
      " VERB_CHILD_DEP_NPADVMOD = 2                 pos : neg    =      8.1 : 1.0\n",
      "    VERB_CHILD_DEP_APPOS = 2                 pos : neg    =      8.1 : 1.0\n",
      "     VERB_HEAD_DEP_APPOS = 3                 pos : neg    =      8.1 : 1.0\n",
      "     VERB_CHILD_DEP_PREP = 5                 pos : neg    =      8.1 : 1.0\n",
      "     VERB_CHILD_DEP_DOBJ = 6                 pos : neg    =      8.1 : 1.0\n",
      "       VERB_CHILD_POS_IN = 6                 pos : neg    =      8.1 : 1.0\n",
      "     VERB_CHILD_DEP_ATTR = 1                 neg : pos    =      7.8 : 1.0\n",
      "      VERB_HEAD_DEP_ATTR = 1                 neg : pos    =      7.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Naive Bayes'\")\n",
    "classifier.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC's\n",
      "Most Informative Features\n",
      "\t-1.3291\tVERB_HEAD_DEP_NPADVMOD\t\t1.3190\tFOLLOWING_MD   \n",
      "\t-1.2536\tVERB_HEAD_POS_IN\t\t1.2541\tVERB_HEAD_POS_NNP\n",
      "\t-1.1268\tVERB_HEAD_POS_CD\t\t1.0593\tFOLLOWING_PDT  \n",
      "\t-0.9553\tVERB_CHILD_POS_WDT\t\t1.0263\tVERB_HEAD_DEP_OPRD\n",
      "\t-0.9482\tFOLLOWING_HYPH \t\t0.9821\tVERB_HEAD_DEP_ADVMOD||CONJ\n",
      "\t-0.9174\tFOLLOWING_JJR  \t\t0.9790\tVERB_CHILD_POS_-RRB-\n",
      "\t-0.8975\tVERB_CHILD_DEP_AGENT\t\t0.7315\tFOLLOWING_CD   \n",
      "\t-0.8735\tVERB_HEAD_POS_RB\t\t0.7277\tVERB_HEAD_DEP_CSUBJ\n",
      "\t-0.8319\tVERB_HEAD_DEP_DET\t\t0.6689\tVERB_CHILD_POS_NNP\n",
      "\t-0.8150\tVERB_CHILD_POS_POS\t\t0.6601\tVERB_CHILD_DEP_AMOD\n",
      "\t-0.8150\tFOLLOWING_POS  \t\t0.6315\tVERB           \n",
      "\t-0.8147\tVERB_CHILD_POS_HYPH\t\t0.5917\tVERB_CHILD_POS_PDT\n",
      "\t-0.7742\tVERB_CHILD_DEP_NMOD\t\t0.5542\tVERB_HEAD_DEP_ADVMOD\n",
      "\t-0.7741\tVERB_CHILD_POS_``\t\t0.5487\tVERB_CHILD_DEP_NPADVMOD\n",
      "\t-0.7512\tVERB_HEAD_POS_UH\t\t0.5190\tVERB_CHILD_DEP_MARK\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/a/11140887\n",
    "def show_most_informative_features(vectorizer, clf, n=20):\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))\n",
    "    top = zip(coefs_with_fns[:n], coefs_with_fns[:-(n + 1):-1])\n",
    "    for (coef_1, fn_1), (coef_2, fn_2) in top:\n",
    "        print(\"\\t%.4f\\t%-15s\\t\\t%.4f\\t%-15s\" % (coef_1, fn_1, coef_2, fn_2))\n",
    "       \n",
    "print('LinearSVC\\'s')\n",
    "print('Most Informative Features')\n",
    "show_most_informative_features(LinearSVC_classifier._vectorizer, LinearSVC_classifier._clf, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things abandoned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I needed a library that supports dependency parsing, which NLTK does not... so I thought I'd add the [Stanford CoreNLP](https://stanfordnlp.github.io/CoreNLP/) toolkit and [its associated software](https://nlp.stanford.edu/software/) to NLTK. However, there are many conflicting instructions for installing the Java-based project, depending on NLTK version used. By the time I figured this out, the installation had become a time sink. So I abandoned this effort in favor of Spacy.io.\n",
    "\n",
    "I might return this way if I want to improve results/implement a voter system between the various linguistic and classification methods later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = [s for l in lines for s in sent_tokenize(l)] # punkt\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagged_sentences = []\n",
    "for s in sentences:\n",
    "    words = word_tokenize(s)\n",
    "    tagged = nltk.pos_tag(words) # averaged_perceptron_tagger\n",
    "    tagged_sentences.append(tagged)\n",
    "print(tagged_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: POS accuracy\n",
    "\n",
    "`Run down to the shop, will you, Peter` is parsed unexpectedly by `nltk.pos_tag`:\n",
    "> `[('Run', 'NNP'), ('down', 'RB'), ('to', 'TO'), ('the', 'DT'), ('shop', 'NN'), (',', ','), ('will', 'MD'), ('you', 'PRP'), (',', ','), ('Peter', 'NNP')]`\n",
    "\n",
    "`Run` is tagged as a `NNP (proper noun, singular)`\n",
    "\n",
    "I expected an output more like what the [Stanford Parser](http://nlp.stanford.edu:8080/parser/) provides:\n",
    "> `Run/VBG down/RP to/TO the/DT shop/NN ,/, will/MD you/PRP ,/, Peter/NNP`\n",
    "\n",
    "`Run` is tagged as a `VGB (verb, gerund/present participle)` - still not quite the `VB` I want, but at least it's a `V*`\n",
    "\n",
    "_MEANWHILE..._\n",
    "\n",
    "`nltk.pos_tag` did better with:\n",
    "> `[('Do', 'VB'), ('not', 'RB'), ('clean', 'VB'), ('soot', 'NN'), ('off', 'IN'), ('the', 'DT'), ('window', 'NN')]`\n",
    "\n",
    "Compared to [Stanford CoreNLP](http://nlp.stanford.edu:8080/corenlp/process) (note that this is different than what [Stanford Parser](http://nlp.stanford.edu:8080/parser/) outputs):\n",
    "> `(ROOT (S (VP (VB Do) (NP (RB not) (JJ clean) (NN soot)) (PP (IN off) (NP (DT the) (NN window))))))`\n",
    "\n",
    "Concern: _clean_ as `VB (verb, base form)` vs `JJ (adjective)` \n",
    "\n",
    "**IMPROVE** POS taggers should vote: nltk.pos_tag (averaged_perceptron_tagger), Stanford Parser, CoreNLP, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Featurization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "featuresets = []\n",
    "for ts in tagged_sentences:\n",
    "    s_features = defaultdict(int)\n",
    "    for idx, tup in enumerate(ts):\n",
    "        #print(tup)\n",
    "        pos = tup[1]\n",
    "        # FeatureName.VERB\n",
    "        is_verb = re.match(r'VB.?', pos) is not None\n",
    "        print(tup, is_verb)\n",
    "        if is_verb:\n",
    "            s_features[FeatureName.VERB] += 1\n",
    "            # FOLLOWING_POS\n",
    "            next_idx = idx + 1;\n",
    "            if next_idx < len(ts):\n",
    "                s_features[f'{FeatureName.FOLLOWING}_{ts[next_idx][1]}'] += 1\n",
    "            # VERB_MODIFIER\n",
    "            # VERB_MODIFYING\n",
    "        else:\n",
    "            s_features[FeatureName.VERB] = 0\n",
    "    featuresets.append(dict(s_features))\n",
    "\n",
    "print()\n",
    "print(featuresets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Stanford NLP](https://nlp.stanford.edu/software/)\n",
    "Setup guide used: https://stackoverflow.com/a/34112695"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get dependency parser, NER, POS tagger\n",
    "!wget https://nlp.stanford.edu/software/stanford-parser-full-2017-06-09.zip\n",
    "!wget https://nlp.stanford.edu/software/stanford-ner-2017-06-09.zip\n",
    "!wget https://nlp.stanford.edu/software/stanford-postagger-full-2017-06-09.zip\n",
    "!unzip stanford-parser-full-2017-06-09.zip\n",
    "!unzip stanford-ner-2017-06-09.zip\n",
    "!unzip stanford-postagger-full-2017-06-09.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.parse.stanford import StanfordParser\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "from nltk.parse.stanford import StanfordNeuralDependencyParser\n",
    "from nltk.tag.stanford import StanfordPOSTagger, StanfordNERTagger\n",
    "from nltk.tokenize.stanford import StanfordTokenizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
