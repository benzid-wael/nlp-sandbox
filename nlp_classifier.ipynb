{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP for Task Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hypothesis**: Part of Speech (POS) tagging and syntactic dependency parsing provides valuable information for classifying imperative phrases. The thinking is that being able to detect imperative phrases will transfer well to detecting tasks and to-dos.\n",
    "\n",
    "#### Some Terminology\n",
    "- [_Imperative mood_](https://en.wikipedia.org/wiki/Imperative_mood) is \"used principally for ordering, requesting or advising the listener to do (or not to do) something... also often used for giving instructions as to how to perform a task.\"\n",
    "- _Part of speech (POS)_ is a way of categorizing a word based on its syntactic function.\n",
    "    - The POS tagger from Spacy.io that is used in this notebook differentiates between [*pos_* and *tag_*](https://spacy.io/docs/api/annotation#pos-tagging-english) - *POS (pos_)* refers to \"coarse-grained part-of-speech\" like `VERB`, `ADJ`, or `PUNCT`; and *POSTAG (tag_)* refers to \"fine-grained part-of-speech\" like `VB`, `JJ`, or `.`.\n",
    "- _Syntactic dependency parsing_ is a way of connecting words based on syntactic relationships, [such as](https://spacy.io/docs/api/annotation#dependency-parsing-english) `DOBJ` (direct object), `PREP` (prepositional modifier), or `POBJ` (object of preposition).\n",
    "    - Check out the dependency parse of the phrase [\"Send the report to Kyle by tomorrow\"](https://demos.explosion.ai/displacy/?text=Send%20the%20report%20to%20Kyle%20by%20tomorrow&model=en&cpu=1&cph=1) as an example.\n",
    "\n",
    "### Proposed Features\n",
    "The imperative mood centers around _actions_, and actions are generally represented in English using verbs. So the features are engineered to also center on the VERB:\n",
    "1. `FeatureName.VERB`: Does the phrase contain `VERB`(s) of the tag form `VB*`?\n",
    "2. `FeatureName.FOLLOWING_POS`: Are the words following the `VERB`(s) of certain parts of speech?\n",
    "3. `FeatureName.FOLLOWING_POSTAG`: Are the words following the `VERB`(s) of certain POS tags?\n",
    "4. `FeatureName.CHILD_DEP`: Are the `VERB`(s) parents of certain syntactic dependencies?\n",
    "5. `FeatureName.PARENT_DEP`: Are the `VERB`(s) children of certain syntactic dependencies?\n",
    "6. `FeatureName.CHILD_POS`: Are the syntactic dependencies that the `VERB`(s) are children of of certain parts of speech?\n",
    "7. `FeatureName.CHILD_POSTAG`: Are the syntactic dependencies that the `VERB`(s) are children of of certain POS tags?\n",
    "8. `FeatureName.PARENT_POS`: Are the syntactic dependencies that the `VERB`(s) parent of certain parts of speech?\n",
    "9. `FeatureName.PARENT_POSTAG`: Are the syntactic dependencies that the `VERB`(s) parent of certain POS tags?\n",
    "\n",
    "**Notes:**\n",
    "- Features 2-9 all depend on feature 1 between `True`; if `False`, phrase vectorization will result in all zeroes.\n",
    "- When features 2-9 are applied to actual phrases, they will append identifying informating about the feature in the form of `_*` (e.g., `FeatureName.FOLLOWING_POSTAG_WRB`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a recipe corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wrote and ran `epicurious_recipes.py`\\* to scrape Epicurious.com for recipe instructions and descriptions. I then performed some manual cleanup of the script results. Output is in `epicurious-pos.txt` and `epicurious-neg.txt`.\n",
    "\n",
    "\\* _script (very) loosely based off of https://github.com/benosment/hrecipe-parse_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** that deriving all negative examples in the training set from Epicurious recipe descriptions would result in negative examples that are longer and syntactically more complicated than the positive examples. This is a form of bias.\n",
    "\n",
    "To (hopefully?) correct for this a bit, I will add the short movie reviews found at https://pythonprogramming.net/static/downloads/short_reviews/ as more negative examples.\n",
    "\n",
    "This still feels weird because we're selecting negative examples only from specific categories of text (recipe descriptions, short movie reviews) - just because they're readily available. Further, most positive examples are recipe instructions - also a specific (and not necessarily related to the main \"task\" category) category of text.\n",
    "\n",
    "Ultimately though, this recipe corpus is a **stopgap/proof of concept** for a corpus more relevant to tasks later on, so I won't worry further about this for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from numpy import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BASE_DIR = os.getcwd()\n",
    "pos_data_path = BASE_DIR + '/pos.txt'\n",
    "neg_data_path = BASE_DIR + '/neg.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(pos_data_path, 'r', encoding='utf-8') as f:\n",
    "    pos_data = f.read()\n",
    "with open(neg_data_path, 'r', encoding='utf-8') as f:\n",
    "    neg_data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_data_split = pos_data.split('\\n')\n",
    "neg_data_split = neg_data.split('\\n')\n",
    "\n",
    "num_pos = len(pos_data_split)\n",
    "num_neg = len(neg_data_split)\n",
    "\n",
    "# 50/50 split between the number of positive and negative samples\n",
    "num = num_pos if num_pos < num_neg else num_neg\n",
    "\n",
    "# shuffle samples\n",
    "random.shuffle(pos_data_split)\n",
    "random.shuffle(neg_data_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines = []\n",
    "for l in pos_data_split[:num]:\n",
    "    lines.append((l, 'pos'))\n",
    "for l in neg_data_split[:num]:\n",
    "    lines.append((l, 'neg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Features as defined in the introduction\n",
    "from enum import Enum, auto\n",
    "class FeatureName(Enum):\n",
    "    VERB = auto()\n",
    "    FOLLOWING_POS = auto()\n",
    "    FOLLOWING_POSTAG = auto()\n",
    "    CHILD_DEP = auto()\n",
    "    PARENT_DEP = auto()\n",
    "    CHILD_POS = auto()\n",
    "    CHILD_POSTAG = auto()\n",
    "    PARENT_POS = auto()\n",
    "    PARENT_POSTAG = auto()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [spaCy.io](https://spacy.io/) for NLP\n",
    "_Because Stanford CoreNLP is hard to install for Python_\n",
    "\n",
    "Found Spacy through an article on [\"Training a Classifier for Relation Extraction from Medical Literature\"](https://www.microsoft.com/developerblog/2016/09/13/training-a-classifier-for-relation-extraction-from-medical-literature/) ([GitHub](https://github.com/CatalystCode/corpus-to-graph-ml))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"nltk_library_comparison.png\" alt=\"NLTK library comparison chart https://spacy.io/docs/api/#comparison\" style=\"width: 400px; margin: 0;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!conda config --add channels conda-forge\n",
    "#!conda install spacy\n",
    "#!python -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Spacy Data Model for NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy's sentence segmentation is lacking... https://github.com/explosion/spaCy/issues/235. So each '\\n' will start a new Spacy Doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_spacy_docs(ll):\n",
    "    dd = [(nlp(l[0]), l[1]) for l in ll]\n",
    "    # collapse noun phrases into single compounds\n",
    "    for d in dd:\n",
    "        for np in d[0].noun_chunks:\n",
    "            np.merge(np.root.tag_, np.text, np.root.ent_type_)\n",
    "    return dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs = create_spacy_docs(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization, POS tagging, and dependency parsing happened automatically with the `nlp(line)` calls above! So let's look at the outputs.\n",
    "\n",
    "https://spacy.io/docs/usage/data-model and https://spacy.io/docs/api/doc will be useful going forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Add tofu and gently toss; set aside to marinate.]\n",
      "[Scatter cheese on top. Finely grate some lemon zest over salad, then slice open lemon and squeeze on some juice.]\n",
      "[Toss zucchini, vinegar, chopped oregano, and 1/4 cup oil in a medium bowl to combine; season zucchini salsa with salt and pepper.]\n",
      "[Berry syrup can be made 3 days ahead.]\n",
      "[Cook, stirring continuously and occasionally scraping the bottom of the pan with the wooden spoon, until the mixture has thickened and coats the spoon with little to no transparency.]\n",
      "[Remove seeds from remaining 2 tomatoes and cut into 1/4\" cubes.]\n",
      "[Transfer to a cutting board and let cool slightly.]\n",
      "[Combine grated garlic, lemon juice, and a pinch of salt in a food processor and let sit until the bite in garlic mellows, about 5 minutes.]\n",
      "[Mix at low speed just until the ingredients come together, about 1 minute.]\n",
      "[Serve or freeze in an airtight container for up to a week.]\n"
     ]
    }
   ],
   "source": [
    "for doc in docs[:10]:\n",
    "    print(list(doc[0].sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tofu]\n",
      "[cheese, top, some lemon zest, salad, some juice]\n",
      "[zucchini, vinegar, oregano, 1/4 cup oil, a medium bowl, salt, pepper]\n",
      "[Berry syrup]\n",
      "[Cook, the bottom, the pan, the wooden spoon, the mixture, the spoon, no transparency]\n",
      "[seeds, 2 tomatoes, 1/4\" cubes]\n",
      "[Transfer, a cutting board]\n",
      "[garlic, lemon juice, a pinch, salt, a food processor, the bite, garlic mellows]\n",
      "[Mix, low speed, the ingredients]\n",
      "[Serve, freeze, an airtight container]\n"
     ]
    }
   ],
   "source": [
    "for doc in docs[:10]:\n",
    "    print(list(doc[0].noun_chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Spacy's dependency graph visualization](https://demos.explosion.ai/displacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add ROOT add VERB VB Add [tofu, toss, .]\n",
      "tofu dobj tofu NOUN NN Add [and]\n",
      "and cc and CCONJ CC tofu []\n",
      "gently advmod gently ADV RB toss []\n",
      "toss conj toss VERB VB Add [gently, ;, set]\n",
      "; punct ; PUNCT : toss []\n",
      "set acl set VERB VBN toss [aside, marinate]\n",
      "aside advmod aside ADV RB set []\n",
      "to aux to PART TO marinate []\n",
      "marinate advcl marinate VERB VB set [to]\n",
      ". punct . PUNCT . Add []\n",
      "Scatter ROOT scatter ADJ JJ Scatter [cheese, ., grate, .]\n",
      "cheese dobj cheese NOUN NN Scatter [on]\n",
      "on prep on ADP IN cheese [top]\n",
      "top pobj top NOUN NN on []\n",
      ". punct . PUNCT . Scatter []\n",
      "Finely advmod finely ADV RB grate []\n",
      "grate conj grate VERB VB Scatter [Finely, some lemon zest]\n",
      "some lemon zest dobj some lemon zest NOUN NN grate [over]\n",
      "over prep over ADP IN some lemon zest [salad]\n",
      "salad pobj salad NOUN NN over [,, lemon]\n",
      ", punct , PUNCT , salad []\n",
      "then advmod then ADV RB slice []\n",
      "slice compound slice NOUN NN lemon [then]\n",
      "open amod open ADJ JJ lemon []\n",
      "lemon appos lemon NOUN NN salad [slice, open, and, squeeze]\n",
      "and cc and CCONJ CC lemon []\n",
      "squeeze conj squeeze NOUN NN lemon [on]\n",
      "on prep on ADP IN squeeze [some juice]\n",
      "some juice pobj some juice NOUN NN on []\n",
      ". punct . PUNCT . Scatter []\n",
      "Toss advcl toss VERB VB chopped [zucchini]\n",
      "zucchini dobj zucchini NOUN NNS Toss [,]\n",
      ", punct , PUNCT , zucchini []\n",
      "vinegar nsubj vinegar NOUN NN chopped [,]\n",
      ", punct , PUNCT , vinegar []\n",
      "chopped ROOT chop VERB VBD chopped [Toss, vinegar, oregano, ,, and, 1/4 cup oil, .]\n",
      "oregano dobj oregano NOUN NN chopped []\n",
      ", punct , PUNCT , chopped []\n",
      "and cc and CCONJ CC chopped []\n",
      "1/4 cup oil conj 1/4 cup oil NOUN NN chopped [in, combine, ;, salsa]\n",
      "in prep in ADP IN 1/4 cup oil [a medium bowl]\n",
      "a medium bowl pobj a medium bowl NOUN NN in []\n",
      "to aux to PART TO combine []\n",
      "combine advcl combine VERB VB 1/4 cup oil [to]\n",
      "; punct ; PUNCT : 1/4 cup oil []\n",
      "season compound season NOUN NN salsa []\n",
      "zucchini compound zucchini NOUN NNS salsa []\n",
      "salsa appos salsa NOUN NN 1/4 cup oil [season, zucchini, with]\n",
      "with prep with ADP IN salsa [salt]\n",
      "salt pobj salt NOUN NN with [and, pepper]\n",
      "and cc and CCONJ CC salt []\n",
      "pepper conj pepper NOUN NN salt []\n",
      ". punct . PUNCT . chopped []\n",
      "Berry syrup nsubjpass Berry syrup NOUN NN made []\n",
      "can aux can VERB MD made []\n",
      "be auxpass be VERB VB made []\n",
      "made ROOT make VERB VBN made [Berry syrup, can, be, ahead, .]\n",
      "3 nummod 3 NUM CD days []\n",
      "days npadvmod day NOUN NNS ahead [3]\n",
      "ahead advmod ahead ADV RB made [days]\n",
      ". punct . PUNCT . made []\n",
      "Cook ROOT Cook PROPN NNP Cook [,, scraping]\n",
      ", punct , PUNCT , Cook []\n",
      "stirring advcl stir VERB VBG scraping []\n",
      "continuously advmod continuously ADV RB scraping [and]\n",
      "and cc and CCONJ CC continuously []\n",
      "occasionally advmod occasionally ADV RB scraping []\n",
      "scraping appos scrap VERB VBG Cook [stirring, continuously, occasionally, the bottom, thickened, coats, .]\n",
      "the bottom dobj the bottom NOUN NN scraping [of]\n",
      "of prep of ADP IN the bottom [the pan]\n",
      "the pan pobj the pan NOUN NN of [with]\n",
      "with prep with ADP IN the pan [the wooden spoon]\n",
      "the wooden spoon pobj the wooden spoon NOUN NN with [,]\n",
      ", punct , PUNCT , the wooden spoon []\n",
      "until mark until ADP IN thickened []\n",
      "the mixture nsubj the mixture NOUN NN thickened []\n",
      "has aux have VERB VBZ thickened []\n",
      "thickened advcl thicken VERB VBN scraping [until, the mixture, has, and]\n",
      "and cc and CCONJ CC thickened []\n",
      "coats conj coat NOUN NNS scraping [the spoon]\n",
      "the spoon dobj the spoon NOUN NN coats [with]\n",
      "with prep with ADP IN the spoon [to]\n",
      "little amod little ADJ JJ to []\n",
      "to prep to PART TO with [little, no transparency]\n",
      "no transparency pobj no transparency NOUN NN to []\n",
      ". punct . PUNCT . scraping []\n"
     ]
    }
   ],
   "source": [
    "for doc in docs[:5]:\n",
    "    for token in doc[0]:\n",
    "        print(token.text, token.dep_, token.lemma_, token.pos_, token.tag_, token.head, list(token.children))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Featurization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def featurize(d):\n",
    "    s_features = defaultdict(int)\n",
    "    for idx, token in enumerate(d):\n",
    "        if re.match(r'VB.?', token.tag_) is not None: # note: not using token.pos == VERB because this also includes BES, HVS, MD tags \n",
    "            s_features[FeatureName.VERB.name] += 1\n",
    "            # FOLLOWING_POS\n",
    "            # FOLLOWING_POSTAG\n",
    "            next_idx = idx + 1;\n",
    "            if next_idx < len(d):\n",
    "                s_features[f'{FeatureName.FOLLOWING_POS.name}_{d[next_idx].pos_}'] += 1\n",
    "                s_features[f'{FeatureName.FOLLOWING_POSTAG.name}_{d[next_idx].tag_}'] += 1\n",
    "            # PARENT_DEP\n",
    "            # PARENT_POS\n",
    "            # PARENT_POSTAG\n",
    "            '''\n",
    "            \"Because the syntactic relations form a tree, every word has exactly one head.\n",
    "            You can therefore iterate over the arcs in the tree by iterating over the words in the sentence.\"\n",
    "            https://spacy.io/docs/usage/dependency-parse#navigating\n",
    "            '''\n",
    "            if (token.head is not token):\n",
    "                s_features[f'{FeatureName.PARENT_DEP.name}_{token.head.dep_.upper()}'] += 1\n",
    "                s_features[f'{FeatureName.PARENT_POS.name}_{token.head.pos_}'] += 1\n",
    "                s_features[f'{FeatureName.PARENT_POSTAG.name}_{token.head.tag_}'] += 1\n",
    "            # CHILD_DEP\n",
    "            # CHILD_POS\n",
    "            # CHILD_POSTAG\n",
    "            for child in token.children:\n",
    "                s_features[f'{FeatureName.CHILD_DEP.name}_{child.dep_.upper()}'] += 1\n",
    "                s_features[f'{FeatureName.CHILD_POS.name}_{child.pos_}'] += 1\n",
    "                s_features[f'{FeatureName.CHILD_POSTAG.name}_{child.tag_}'] += 1\n",
    "    return dict(s_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featuresets = [(doc[0], (featurize(doc[0]), doc[1])) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats on number of features per example:\n",
      "mean: 22.88600697471665\n",
      "stdev: 14.543271036053682\n",
      "median: 23.0\n",
      "mode: 0\n",
      "max: 75\n",
      "min: 0\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean, median, mode, stdev\n",
    "f_lengths = [len(fs[1][0]) for fs in featuresets]\n",
    "\n",
    "print('Stats on number of features per example:')\n",
    "print(f'mean: {mean(f_lengths)}')\n",
    "print(f'stdev: {stdev(f_lengths)}')\n",
    "print(f'median: {median(f_lengths)}')\n",
    "print(f'mode: {mode(f_lengths)}')\n",
    "print(f'max: {max(f_lengths)}')\n",
    "print(f'min: {min(f_lengths)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Add tofu and gently toss; set aside to marinate.,\n",
       "  ({'CHILD_DEP_ACL': 1,\n",
       "    'CHILD_DEP_ADVCL': 1,\n",
       "    'CHILD_DEP_ADVMOD': 2,\n",
       "    'CHILD_DEP_AUX': 1,\n",
       "    'CHILD_DEP_CONJ': 1,\n",
       "    'CHILD_DEP_DOBJ': 1,\n",
       "    'CHILD_DEP_PUNCT': 2,\n",
       "    'CHILD_POSTAG_.': 1,\n",
       "    'CHILD_POSTAG_:': 1,\n",
       "    'CHILD_POSTAG_NN': 1,\n",
       "    'CHILD_POSTAG_RB': 2,\n",
       "    'CHILD_POSTAG_TO': 1,\n",
       "    'CHILD_POSTAG_VB': 2,\n",
       "    'CHILD_POSTAG_VBN': 1,\n",
       "    'CHILD_POS_ADV': 2,\n",
       "    'CHILD_POS_NOUN': 1,\n",
       "    'CHILD_POS_PART': 1,\n",
       "    'CHILD_POS_PUNCT': 2,\n",
       "    'CHILD_POS_VERB': 3,\n",
       "    'FOLLOWING_POSTAG_.': 1,\n",
       "    'FOLLOWING_POSTAG_:': 1,\n",
       "    'FOLLOWING_POSTAG_NN': 1,\n",
       "    'FOLLOWING_POSTAG_RB': 1,\n",
       "    'FOLLOWING_POS_ADV': 1,\n",
       "    'FOLLOWING_POS_NOUN': 1,\n",
       "    'FOLLOWING_POS_PUNCT': 2,\n",
       "    'PARENT_DEP_ACL': 1,\n",
       "    'PARENT_DEP_CONJ': 1,\n",
       "    'PARENT_DEP_ROOT': 1,\n",
       "    'PARENT_POSTAG_VB': 2,\n",
       "    'PARENT_POSTAG_VBN': 1,\n",
       "    'PARENT_POS_VERB': 3,\n",
       "    'VERB': 4},\n",
       "   'pos')),\n",
       " (Scatter cheese on top. Finely grate some lemon zest over salad, then slice open lemon and squeeze on some juice.,\n",
       "  ({'CHILD_DEP_ADVMOD': 1,\n",
       "    'CHILD_DEP_DOBJ': 1,\n",
       "    'CHILD_POSTAG_NN': 1,\n",
       "    'CHILD_POSTAG_RB': 1,\n",
       "    'CHILD_POS_ADV': 1,\n",
       "    'CHILD_POS_NOUN': 1,\n",
       "    'FOLLOWING_POSTAG_NN': 1,\n",
       "    'FOLLOWING_POS_NOUN': 1,\n",
       "    'PARENT_DEP_ROOT': 1,\n",
       "    'PARENT_POSTAG_JJ': 1,\n",
       "    'PARENT_POS_ADJ': 1,\n",
       "    'VERB': 1},\n",
       "   'pos'))]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featuresets[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On one run, the above line printed the following featureset:\n",
    "`(Gather foil loosely on top and bake for 1 1/2 hours., ({}, 'pos'))`\n",
    "\n",
    "This is because the Spacy.io POS tagger provided this:\n",
    "   `Gather/NNP foil/NN loosely/RB on/IN top/NN and/CC bake/NN for/IN 1 1/2 hours./NNS`\n",
    "\n",
    "...with no VERBs tagged, which is incorrect.\n",
    "\n",
    "\"Voting - POS taggers and classifiers\" in the _Next Steps/Improvements_ section below is meant to improve on this.\n",
    "\n",
    "---\n",
    "Compare to [Stanford CoreNLP POS tagger](http://nlp.stanford.edu:8080/corenlp/process):\n",
    "   `Gather/VB foil/NN loosely/RB on/IN top/JJ and/CC bake/VB for/IN 1 1/2/CD hours/NNS ./.`\n",
    "\n",
    "And [Stanford Parser](http://nlp.stanford.edu:8080/parser/index.jsp):\n",
    "   `Gather/NNP foil/VB loosely/RB on/IN top/NN and/CC bake/VB for/IN 1 1/2/CD hours/NNS ./.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training samples: 3670\n",
      "# test samples: 918\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(featuresets)\n",
    "\n",
    "split_num = round(num*2 / 5)\n",
    "\n",
    "# train and test sets\n",
    "testing_set = [fs[1] for i, fs in enumerate(featuresets[:split_num])]\n",
    "training_set =  [fs[1] for i, fs in enumerate(featuresets[split_num:])]\n",
    "\n",
    "print(f'# training samples: {len(training_set)}')\n",
    "print(f'# test samples: {len(testing_set)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# decoupling the functionality of nltk.classify.accuracy\n",
    "def predict(classifier, gold, prob=True):\n",
    "    if (prob is True):\n",
    "        predictions = classifier.prob_classify_many([fs for (fs, ll) in gold])\n",
    "    else:\n",
    "        predictions = classifier.classify_many([fs for (fs, ll) in gold])\n",
    "    return list(zip(predictions, [ll for (fs, ll) in gold]))\n",
    "\n",
    "def accuracy(predicts, prob=True):\n",
    "    if (prob is True):\n",
    "        correct = [label == prediction.max() for (prediction, label) in predicts]\n",
    "    else:\n",
    "        correct = [label == prediction for (prediction, label) in predicts]\n",
    "        \n",
    "    if correct:\n",
    "        return sum(correct) / len(correct)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note below the use of `DummyClassifier` to provide a simple sanity check, a baseline of random predictions. `stratified` means it \"generates random predictions by respecting the training set class distribution.\" (http://scikit-learn.org/stable/modules/model_evaluation.html#dummy-estimators)\n",
    "\n",
    "> More generally, when the accuracy of a classifier is too close to random, it probably means that something went wrong: features are not helpful, a hyperparameter is not correctly tuned, the classifier is suffering from class imbalance, etc…\n",
    "\n",
    "If a classifier can beat the `DummyClassifier`, it is at least learning something valuable! How valuable is another question..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy classifier accuracy percent: 50.76252723311547\n",
      "NaiveBayes classifier accuracy percent: 67.42919389978213\n",
      "MultinomialNB classifier accuracy percent: 79.41176470588235\n",
      "BernoulliNB classifier accuracy percent: 75.92592592592592\n",
      "LogisticRegression classifier accuracy percent: 83.00653594771242\n",
      "SGD classifier accuracy percent: 81.59041394335512\n",
      "SVC classifier accuracy percent: 81.48148148148148\n",
      "LinearSVC classifier accuracy percent: 83.76906318082789\n",
      "DecisionTree classifier accuracy percent: 79.30283224400871\n",
      "RandomForest classifier accuracy percent: 81.91721132897604\n"
     ]
    }
   ],
   "source": [
    "from nltk import NaiveBayesClassifier\n",
    "from nltk.classify.decisiontree import DecisionTreeClassifier\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "dummy = SklearnClassifier(DummyClassifier(strategy='stratified', random_state=0))\n",
    "dummy.train(training_set)\n",
    "dummy_predict = predict(dummy, testing_set)\n",
    "dummy_accuracy = accuracy(dummy_predict)\n",
    "print(\"Dummy classifier accuracy percent:\", dummy_accuracy*100)\n",
    "\n",
    "nb = NaiveBayesClassifier.train(training_set)\n",
    "nb_predict = predict(nb, testing_set)\n",
    "nb_accuracy = accuracy(nb_predict)\n",
    "print(\"NaiveBayes classifier accuracy percent:\", nb_accuracy*100)\n",
    "\n",
    "multinomial_nb = SklearnClassifier(MultinomialNB())\n",
    "multinomial_nb.train(training_set)\n",
    "mnb_predict = predict(multinomial_nb, testing_set)\n",
    "mnb_accuracy = accuracy(mnb_predict)\n",
    "print(\"MultinomialNB classifier accuracy percent:\", mnb_accuracy*100)\n",
    "\n",
    "bernoulli_nb = SklearnClassifier(BernoulliNB())\n",
    "bernoulli_nb.train(training_set)\n",
    "bnb_predict = predict(bernoulli_nb, testing_set)\n",
    "bnb_accuracy = accuracy(bnb_predict)\n",
    "print(\"BernoulliNB classifier accuracy percent:\", bnb_accuracy*100)\n",
    "\n",
    "# ??logistic_regression._clf\n",
    "#   sklearn.svm.LinearSVC : learns SVM models using the same algorithm.\n",
    "logistic_regression = SklearnClassifier(LogisticRegression())\n",
    "logistic_regression.train(training_set)\n",
    "lr_predict = predict(logistic_regression, testing_set)\n",
    "lr_accuracy = accuracy(lr_predict)\n",
    "print(\"LogisticRegression classifier accuracy percent:\", lr_accuracy*100)\n",
    "\n",
    "# ??sgd._clf\n",
    "#    The 'log' loss gives logistic regression, a probabilistic classifier.\n",
    "# ??linear_svc._clf\n",
    "#   can optimize the same cost function as LinearSVC\n",
    "#   by adjusting the penalty and loss parameters. In addition it requires\n",
    "#   less memory, allows incremental (online) learning, and implements\n",
    "#   various loss functions and regularization regimes.\n",
    "sgd = SklearnClassifier(SGDClassifier(loss='log'))\n",
    "sgd.train(training_set)\n",
    "sgd_predict = predict(sgd, testing_set)\n",
    "sgd_accuracy = accuracy(sgd_predict)\n",
    "print(\"SGD classifier accuracy percent:\", sgd_accuracy*100)\n",
    "\n",
    "# slow\n",
    "# using libsvm with kernel 'rbf' (radial basis function)\n",
    "svc = SklearnClassifier(SVC(probability=True))\n",
    "svc.train(training_set)\n",
    "svc_predict = predict(svc, testing_set)\n",
    "svc_accuracy = accuracy(svc_predict)\n",
    "print(\"SVC classifier accuracy percent:\", svc_accuracy*100)\n",
    "\n",
    "# ??linear_svc._clf\n",
    "#    Similar to SVC with parameter kernel='linear', but implemented in terms of\n",
    "#    liblinear rather than libsvm, so it has more flexibility in the choice of\n",
    "#    penalties and loss functions and should scale better to large numbers of\n",
    "#    samples.\n",
    "#    Prefer dual=False when n_samples > n_features.\n",
    "linear_svc = SklearnClassifier(LinearSVC(dual=False))\n",
    "linear_svc.train(training_set)\n",
    "linear_svc_predict = predict(linear_svc, testing_set, False)\n",
    "linear_svc_accuracy = accuracy(linear_svc_predict, False)\n",
    "print(\"LinearSVC classifier accuracy percent:\", linear_svc_accuracy*100)\n",
    "\n",
    "# slower\n",
    "dt = DecisionTreeClassifier.train(training_set)\n",
    "dt_predict = predict(dt, testing_set, False)\n",
    "dt_accuracy = accuracy(dt_predict, False)\n",
    "print(\"DecisionTree classifier accuracy percent:\", dt_accuracy*100)\n",
    "\n",
    "random_forest = SklearnClassifier(RandomForestClassifier(n_estimators = 100))\n",
    "random_forest.train(training_set)\n",
    "rf_predict = predict(random_forest, testing_set)\n",
    "rf_accuracy = accuracy(rf_predict)\n",
    "print(\"RandomForest classifier accuracy percent:\", rf_accuracy*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD: Multiple Epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sgd` classifiers improves with epochs. `??sgd._clf` tells us that the default number of epochs `n_iter` is 5. So let's run more epochs. Also not that the training_set shuffle is `True` by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier classifier accuracy percent (epochs: 1000): 83.00653594771242\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "sgd = SklearnClassifier(SGDClassifier(loss='log', n_iter=num_epochs))\n",
    "sgd.train(training_set)\n",
    "sgd_predict = predict(sgd, testing_set)\n",
    "sgd_accuracy = accuracy(sgd_predict)\n",
    "print(f\"SGDClassifier classifier accuracy percent (epochs: {num_epochs}):\", sgd_accuracy*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, 1000 epochs run very quickly! And `SGDClassifier` performance has improved with more iterations.\n",
    "\n",
    "*Also note that we can set `warm_start` to `True` if we want to take advantage of online learning and reuse the solution of the previous call.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to scope analysis down to our top-performing classifiers, which consistently perform with >80% accuracy: `LogisticRegression`, `SVC`, `LinearSVC`, `SGD`, and `RandomForest`.\n",
    "\n",
    "We'll also include `Dummy` as a baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: I considered omitting `LinearSVC` since we have `SVC` also performing with high accuracy, and since `LinearSVC` does not provide probability estimates I rely on a lot during this analysis. However, `LinearSVC` is much faster than `SVC` and is meant to \"scale better to large numbers of samples.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most Informative Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/a/11140887\n",
    "def show_most_informative_features(vectorizer, clf, n=20):\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))\n",
    "    top = zip(coefs_with_fns[:round(n/2)], coefs_with_fns[:-(round(n/2) + 1):-1])\n",
    "    for (coef_1, fn_1), (coef_2, fn_2) in top:\n",
    "        print(\"\\t%.4f\\t%-15s\\t\\t%.4f\\t%-15s\" % (coef_1, fn_1, coef_2, fn_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD\n",
      "\t-2.4606\tCHILD_DEP_AGENT\t\t2.5138\tCHILD_POSTAG_-RRB-\n",
      "\t-2.4429\tFOLLOWING_POSTAG_-RRB-\t\t1.9397\tCHILD_DEP_ADVMOD||XCOMP\n",
      "\t-2.4134\tCHILD_POSTAG_``\t\t1.8529\tPARENT_DEP_CSUBJ\n",
      "\t-2.3004\tCHILD_POSTAG_HYPH\t\t1.7568\tCHILD_DEP_DOBJ||XCOMP\n",
      "\t-2.1355\tCHILD_DEP_INTJ \t\t1.6884\tVERB           \n",
      "\t-2.0063\tFOLLOWING_POSTAG_WRB\t\t1.5498\tFOLLOWING_POSTAG_RB\n",
      "\t-1.9546\tPARENT_DEP_PCOMP\t\t1.4023\tPARENT_DEP_ADVMOD||CONJ\n",
      "\t-1.8537\tPARENT_POSTAG_VBZ\t\t1.3969\tCHILD_DEP_POBJ \n",
      "\n",
      "Logistic Regression\n",
      "\t-1.9269\tCHILD_DEP_AGENT\t\t1.8322\tCHILD_POSTAG_-RRB-\n",
      "\t-1.9033\tCHILD_POSTAG_HYPH\t\t1.3047\tVERB           \n",
      "\t-1.6845\tPARENT_POSTAG_VBZ\t\t1.2598\tCHILD_DEP_NPADVMOD\n",
      "\t-1.4534\tCHILD_DEP_INTJ \t\t1.1426\tFOLLOWING_POSTAG_RB\n",
      "\t-1.4403\tFOLLOWING_POSTAG_-RRB-\t\t1.1084\tPARENT_POS_PROPN\n",
      "\t-1.4358\tCHILD_DEP_NSUBJ\t\t1.1084\tPARENT_POSTAG_NNP\n",
      "\t-1.4301\tPARENT_DEP_PCOMP\t\t1.0633\tCHILD_POSTAG_MD\n",
      "\t-1.4123\tCHILD_POSTAG_``\t\t1.0621\tCHILD_DEP_ADVCL\n",
      "\n",
      "LinearSVC\n",
      "\t-1.3935\tCHILD_POSTAG_``\t\t1.2202\tCHILD_DEP_ADVMOD||XCOMP\n",
      "\t-1.3363\tFOLLOWING_POSTAG_-RRB-\t\t1.2070\tCHILD_DEP_POBJ \n",
      "\t-1.0491\tCHILD_DEP_AGENT\t\t1.0620\tCHILD_POSTAG_-RRB-\n",
      "\t-0.9386\tFOLLOWING_POSTAG_WRB\t\t1.0486\tPARENT_DEP_CSUBJ\n",
      "\t-0.8731\tCHILD_DEP_INTJ \t\t0.9969\tCHILD_POSTAG_RBS\n",
      "\t-0.8702\tCHILD_POSTAG_HYPH\t\t0.9537\tCHILD_DEP_DOBJ||XCOMP\n",
      "\t-0.8037\tPARENT_POSTAG_PRP\t\t0.8780\tPARENT_DEP_NUMMOD\n",
      "\t-0.8037\tPARENT_POS_PRON\t\t0.7642\tVERB           \n"
     ]
    }
   ],
   "source": [
    "print('SGD')\n",
    "show_most_informative_features(sgd._vectorizer, sgd._clf, 15)\n",
    "print()\n",
    "print('Logistic Regression')\n",
    "show_most_informative_features(logistic_regression._vectorizer, logistic_regression._clf, 15)\n",
    "print()\n",
    "print('LinearSVC')\n",
    "show_most_informative_features(linear_svc._vectorizer, linear_svc._clf, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: Because `SVC` is using the nonlinear RBF kernel, we cannot show the most informative features (`coef_ is only available when using a linear kernel`). The same applies for `Random Forest`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'adjective, superlative'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"JJS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Negative coefficients**:\n",
    "- VERB parents [`AGENT`](http://universaldependencies.org/docs/sv/dep/nmod-agent.html): \"used for agents of passive verbs\" - interpreting this to mean that _existence of passive verbs (i.e., the opposite of active verbs) means negative correlation with it being imperative_\n",
    "- VERB followed by a `WRB`: \"wh-adverb\" (where, when)\n",
    "- VERB is a child of [`AMOD`](http://universaldependencies.org/en/dep/amod.html): \"any adjective or adjectival phrase that serves to modify the meaning\" of the verb\n",
    "\n",
    "\n",
    "**Positive coefficients**:\n",
    "- VERB parents a `-RRB-`: \"right round bracket\"\n",
    "- VERB is a child of `PROPN`: \"proper noun\"\n",
    "- VERB is a child of `NNP`: \"noun, proper singular\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Random Forest` has another built-in way of determining \"feature importance\".\n",
    "\n",
    "**TODO**: How to maps feature #s to feature names?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEICAYAAABS0fM3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6W4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x29f7cd90390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. feature 32 (0.130005)\n",
      "2. feature 243 (0.044240)\n",
      "3. feature 238 (0.039566)\n",
      "4. feature 98 (0.035817)\n",
      "5. feature 107 (0.022575)\n",
      "6. feature 76 (0.022413)\n",
      "7. feature 91 (0.021481)\n",
      "8. feature 104 (0.020507)\n",
      "9. feature 70 (0.019955)\n",
      "10. feature 11 (0.018632)\n",
      "11. feature 264 (0.017018)\n",
      "12. feature 47 (0.015088)\n",
      "13. feature 23 (0.015034)\n",
      "14. feature 92 (0.013961)\n",
      "15. feature 109 (0.013490)\n",
      "16. feature 54 (0.013486)\n",
      "17. feature 12 (0.013276)\n",
      "18. feature 86 (0.013001)\n",
      "19. feature 63 (0.012677)\n",
      "20. feature 99 (0.012374)\n",
      "21. feature 112 (0.012132)\n",
      "22. feature 27 (0.011459)\n",
      "23. feature 162 (0.011128)\n",
      "24. feature 42 (0.011032)\n",
      "25. feature 3 (0.010996)\n",
      "26. feature 2 (0.010728)\n",
      "27. feature 262 (0.010429)\n",
      "28. feature 64 (0.010033)\n",
      "29. feature 100 (0.009873)\n",
      "30. feature 210 (0.009828)\n",
      "31. feature 255 (0.009780)\n",
      "32. feature 78 (0.008624)\n",
      "33. feature 132 (0.008554)\n",
      "34. feature 226 (0.008397)\n",
      "35. feature 4 (0.008100)\n",
      "36. feature 68 (0.007814)\n",
      "37. feature 73 (0.007493)\n",
      "38. feature 126 (0.006799)\n",
      "39. feature 18 (0.006382)\n",
      "40. feature 201 (0.006177)\n",
      "41. feature 31 (0.006172)\n",
      "42. feature 167 (0.006134)\n",
      "43. feature 173 (0.006133)\n",
      "44. feature 157 (0.006096)\n",
      "45. feature 29 (0.006041)\n",
      "46. feature 106 (0.005978)\n",
      "47. feature 158 (0.005777)\n",
      "48. feature 140 (0.005249)\n",
      "49. feature 84 (0.005249)\n",
      "50. feature 16 (0.005241)\n",
      "51. feature 185 (0.005120)\n",
      "52. feature 259 (0.005056)\n",
      "53. feature 221 (0.004981)\n",
      "54. feature 53 (0.004878)\n",
      "55. feature 118 (0.004828)\n",
      "56. feature 51 (0.004802)\n",
      "57. feature 52 (0.004791)\n",
      "58. feature 93 (0.004480)\n",
      "59. feature 241 (0.004301)\n",
      "60. feature 249 (0.004202)\n",
      "61. feature 135 (0.004175)\n",
      "62. feature 168 (0.003865)\n",
      "63. feature 102 (0.003843)\n",
      "64. feature 209 (0.003841)\n",
      "65. feature 227 (0.003786)\n",
      "66. feature 90 (0.003740)\n",
      "67. feature 189 (0.003568)\n",
      "68. feature 108 (0.003522)\n",
      "69. feature 228 (0.003460)\n",
      "70. feature 250 (0.003427)\n",
      "71. feature 138 (0.003418)\n",
      "72. feature 149 (0.003406)\n",
      "73. feature 115 (0.003380)\n",
      "74. feature 71 (0.003380)\n",
      "75. feature 7 (0.003370)\n",
      "76. feature 13 (0.003278)\n",
      "77. feature 163 (0.003238)\n",
      "78. feature 88 (0.003231)\n",
      "79. feature 204 (0.003184)\n",
      "80. feature 164 (0.003160)\n",
      "81. feature 15 (0.003135)\n",
      "82. feature 127 (0.003102)\n",
      "83. feature 165 (0.003050)\n",
      "84. feature 49 (0.002979)\n",
      "85. feature 220 (0.002947)\n",
      "86. feature 59 (0.002930)\n",
      "87. feature 156 (0.002922)\n",
      "88. feature 240 (0.002859)\n",
      "89. feature 46 (0.002858)\n",
      "90. feature 57 (0.002734)\n",
      "91. feature 101 (0.002714)\n",
      "92. feature 33 (0.002660)\n",
      "93. feature 179 (0.002629)\n",
      "94. feature 89 (0.002555)\n",
      "95. feature 81 (0.002500)\n",
      "96. feature 144 (0.002468)\n",
      "97. feature 143 (0.002452)\n",
      "98. feature 62 (0.002420)\n",
      "99. feature 242 (0.002358)\n",
      "100. feature 55 (0.002289)\n",
      "101. feature 25 (0.002237)\n",
      "102. feature 35 (0.002200)\n",
      "103. feature 170 (0.002177)\n",
      "104. feature 160 (0.002138)\n",
      "105. feature 121 (0.002111)\n",
      "106. feature 87 (0.002039)\n",
      "107. feature 195 (0.002021)\n",
      "108. feature 60 (0.001977)\n",
      "109. feature 122 (0.001880)\n",
      "110. feature 95 (0.001770)\n",
      "111. feature 21 (0.001760)\n",
      "112. feature 36 (0.001673)\n",
      "113. feature 159 (0.001660)\n",
      "114. feature 183 (0.001563)\n",
      "115. feature 172 (0.001504)\n",
      "116. feature 148 (0.001460)\n",
      "117. feature 58 (0.001455)\n",
      "118. feature 239 (0.001388)\n",
      "119. feature 174 (0.001387)\n",
      "120. feature 177 (0.001384)\n",
      "121. feature 120 (0.001377)\n",
      "122. feature 105 (0.001330)\n",
      "123. feature 211 (0.001151)\n",
      "124. feature 251 (0.001140)\n",
      "125. feature 79 (0.001058)\n",
      "126. feature 188 (0.001054)\n",
      "127. feature 26 (0.001007)\n",
      "128. feature 97 (0.001000)\n",
      "129. feature 141 (0.000980)\n",
      "130. feature 233 (0.000930)\n",
      "131. feature 117 (0.000921)\n",
      "132. feature 65 (0.000828)\n",
      "133. feature 200 (0.000807)\n",
      "134. feature 17 (0.000730)\n",
      "135. feature 66 (0.000726)\n",
      "136. feature 194 (0.000725)\n",
      "137. feature 19 (0.000684)\n",
      "138. feature 125 (0.000655)\n",
      "139. feature 50 (0.000649)\n",
      "140. feature 20 (0.000584)\n",
      "141. feature 5 (0.000565)\n",
      "142. feature 253 (0.000535)\n",
      "143. feature 151 (0.000514)\n",
      "144. feature 9 (0.000514)\n",
      "145. feature 146 (0.000509)\n",
      "146. feature 119 (0.000503)\n",
      "147. feature 234 (0.000487)\n",
      "148. feature 256 (0.000444)\n",
      "149. feature 8 (0.000443)\n",
      "150. feature 208 (0.000440)\n",
      "151. feature 69 (0.000428)\n",
      "152. feature 22 (0.000416)\n",
      "153. feature 199 (0.000394)\n",
      "154. feature 0 (0.000354)\n",
      "155. feature 166 (0.000338)\n",
      "156. feature 218 (0.000332)\n",
      "157. feature 260 (0.000316)\n",
      "158. feature 198 (0.000287)\n",
      "159. feature 217 (0.000280)\n",
      "160. feature 147 (0.000277)\n",
      "161. feature 133 (0.000245)\n",
      "162. feature 113 (0.000243)\n",
      "163. feature 128 (0.000239)\n",
      "164. feature 130 (0.000237)\n",
      "165. feature 224 (0.000231)\n",
      "166. feature 41 (0.000215)\n",
      "167. feature 30 (0.000215)\n",
      "168. feature 182 (0.000214)\n",
      "169. feature 34 (0.000191)\n",
      "170. feature 116 (0.000175)\n",
      "171. feature 176 (0.000173)\n",
      "172. feature 184 (0.000168)\n",
      "173. feature 61 (0.000165)\n",
      "174. feature 154 (0.000154)\n",
      "175. feature 103 (0.000150)\n",
      "176. feature 187 (0.000137)\n",
      "177. feature 39 (0.000124)\n",
      "178. feature 150 (0.000124)\n",
      "179. feature 85 (0.000120)\n",
      "180. feature 223 (0.000118)\n",
      "181. feature 196 (0.000117)\n",
      "182. feature 222 (0.000116)\n",
      "183. feature 180 (0.000116)\n",
      "184. feature 77 (0.000115)\n",
      "185. feature 139 (0.000114)\n",
      "186. feature 48 (0.000109)\n",
      "187. feature 129 (0.000105)\n",
      "188. feature 246 (0.000105)\n",
      "189. feature 28 (0.000105)\n",
      "190. feature 142 (0.000105)\n",
      "191. feature 74 (0.000100)\n",
      "192. feature 131 (0.000093)\n",
      "193. feature 212 (0.000092)\n",
      "194. feature 40 (0.000089)\n",
      "195. feature 24 (0.000084)\n",
      "196. feature 213 (0.000078)\n",
      "197. feature 6 (0.000072)\n",
      "198. feature 80 (0.000068)\n",
      "199. feature 225 (0.000066)\n",
      "200. feature 82 (0.000063)\n",
      "201. feature 191 (0.000061)\n",
      "202. feature 153 (0.000061)\n",
      "203. feature 263 (0.000059)\n",
      "204. feature 14 (0.000054)\n",
      "205. feature 186 (0.000052)\n",
      "206. feature 75 (0.000051)\n",
      "207. feature 190 (0.000045)\n",
      "208. feature 192 (0.000044)\n",
      "209. feature 56 (0.000044)\n",
      "210. feature 247 (0.000041)\n",
      "211. feature 136 (0.000039)\n",
      "212. feature 231 (0.000039)\n",
      "213. feature 67 (0.000039)\n",
      "214. feature 96 (0.000038)\n",
      "215. feature 10 (0.000035)\n",
      "216. feature 155 (0.000034)\n",
      "217. feature 219 (0.000031)\n",
      "218. feature 181 (0.000031)\n",
      "219. feature 134 (0.000030)\n",
      "220. feature 72 (0.000029)\n",
      "221. feature 258 (0.000027)\n",
      "222. feature 43 (0.000027)\n",
      "223. feature 44 (0.000025)\n",
      "224. feature 114 (0.000024)\n",
      "225. feature 206 (0.000023)\n",
      "226. feature 244 (0.000022)\n",
      "227. feature 38 (0.000021)\n",
      "228. feature 110 (0.000021)\n",
      "229. feature 137 (0.000020)\n",
      "230. feature 257 (0.000019)\n",
      "231. feature 193 (0.000019)\n",
      "232. feature 197 (0.000018)\n",
      "233. feature 178 (0.000017)\n",
      "234. feature 145 (0.000016)\n",
      "235. feature 161 (0.000015)\n",
      "236. feature 202 (0.000015)\n",
      "237. feature 245 (0.000010)\n",
      "238. feature 37 (0.000010)\n",
      "239. feature 237 (0.000009)\n",
      "240. feature 45 (0.000009)\n",
      "241. feature 254 (0.000009)\n",
      "242. feature 248 (0.000009)\n",
      "243. feature 235 (0.000006)\n",
      "244. feature 236 (0.000003)\n",
      "245. feature 152 (0.000002)\n",
      "246. feature 94 (0.000002)\n",
      "247. feature 232 (0.000002)\n",
      "248. feature 1 (0.000002)\n",
      "249. feature 171 (0.000001)\n",
      "250. feature 229 (0.000000)\n",
      "251. feature 123 (0.000000)\n",
      "252. feature 261 (0.000000)\n",
      "253. feature 230 (0.000000)\n",
      "254. feature 111 (0.000000)\n",
      "255. feature 214 (0.000000)\n",
      "256. feature 175 (0.000000)\n",
      "257. feature 252 (0.000000)\n",
      "258. feature 124 (0.000000)\n",
      "259. feature 169 (0.000000)\n",
      "260. feature 216 (0.000000)\n",
      "261. feature 215 (0.000000)\n",
      "262. feature 207 (0.000000)\n",
      "263. feature 203 (0.000000)\n",
      "264. feature 83 (0.000000)\n",
      "265. feature 205 (0.000000)\n"
     ]
    }
   ],
   "source": [
    "# http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import std, argsort\n",
    "\n",
    "importances = random_forest._clf.feature_importances_\n",
    "std = std([tree.feature_importances_ for tree in random_forest._clf.estimators_],\n",
    "             axis=0)\n",
    "indices = argsort(importances)[::-1]\n",
    "\n",
    "num_features = len(random_forest._clf.feature_importances_)\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(num_features), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(num_features), indices)\n",
    "plt.xlim([-1, num_features])\n",
    "plt.show()\n",
    "\n",
    "for f in range(num_features):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scikit Learn metrics: Confusion matrix, Classification report, F1 score, Log loss\n",
    "\n",
    "http://scikit-learn.org/stable/modules/model_evaluation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def classification_report(predict, prob=True):\n",
    "    predictions, labels = zip(*predict)\n",
    "    if prob is True:\n",
    "        return metrics.classification_report(labels, [p.max() for p in predictions])\n",
    "    else:\n",
    "        return metrics.classification_report(labels, predictions)\n",
    "\n",
    "def confusion_matrix(predict, prob=True, print_layout=False):\n",
    "    predictions, labels = zip(*predict)\n",
    "    if print_layout is True:\n",
    "        print('Layout\\n[[tn   fp]\\n [fn   tp]]\\n')\n",
    "    if prob is True:\n",
    "        return metrics.confusion_matrix(labels, [p.max() for p in predictions])\n",
    "    else:\n",
    "        return metrics.confusion_matrix(labels, predictions)\n",
    "\n",
    "def log_loss(predict):\n",
    "    predictions, labels = zip(*predict)\n",
    "    return metrics.log_loss(labels, [p.prob('pos') for p in predictions])\n",
    "\n",
    "def roc_auc_score(predict):\n",
    "    predictions, labels = zip(*predict)\n",
    "    # need to convert labels to binary classification of 0 or 1\n",
    "    return metrics.roc_auc_score([1 if l == 'pos' else 0 for l in labels], [p.prob('pos') for p in predictions], average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.90      0.75      0.82       472\n",
      "        pos       0.78      0.91      0.84       446\n",
      "\n",
      "avg / total       0.84      0.83      0.83       918\n",
      "\n",
      "\n",
      "Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.89      0.77      0.82       472\n",
      "        pos       0.79      0.89      0.84       446\n",
      "\n",
      "avg / total       0.84      0.83      0.83       918\n",
      "\n",
      "\n",
      "SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.85      0.77      0.81       472\n",
      "        pos       0.78      0.86      0.82       446\n",
      "\n",
      "avg / total       0.82      0.81      0.81       918\n",
      "\n",
      "\n",
      "LinearSVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.89      0.78      0.83       472\n",
      "        pos       0.79      0.90      0.84       446\n",
      "\n",
      "avg / total       0.84      0.84      0.84       918\n",
      "\n",
      "\n",
      "Random Forest\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.86      0.78      0.82       472\n",
      "        pos       0.79      0.86      0.82       446\n",
      "\n",
      "avg / total       0.82      0.82      0.82       918\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('SGD')\n",
    "print(classification_report(sgd_predict))\n",
    "print()\n",
    "print('Logistic Regression')\n",
    "print(classification_report(lr_predict))\n",
    "print()\n",
    "print('SVC')\n",
    "print(classification_report(svc_predict))\n",
    "print()\n",
    "print('LinearSVC')\n",
    "print(classification_report(linear_svc_predict, False))\n",
    "print()\n",
    "print('Random Forest')\n",
    "print(classification_report(rf_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layout\n",
      "[[tn   fp]\n",
      " [fn   tp]]\n",
      "\n",
      "SGD\n",
      "[[355 117]\n",
      " [ 39 407]]\n",
      "\n",
      "Logistic Regression\n",
      "[[363 109]\n",
      " [ 47 399]]\n",
      "\n",
      "SVC\n",
      "[[365 107]\n",
      " [ 63 383]]\n",
      "\n",
      "LinearSVC\n",
      "[[366 106]\n",
      " [ 43 403]]\n",
      "\n",
      "Random Forest\n",
      "[[367 105]\n",
      " [ 61 385]]\n"
     ]
    }
   ],
   "source": [
    "print('Layout\\n[[tn   fp]\\n [fn   tp]]\\n')\n",
    "\n",
    "print('SGD')\n",
    "print(confusion_matrix(sgd_predict))\n",
    "print()\n",
    "print('Logistic Regression')\n",
    "print(confusion_matrix(lr_predict))\n",
    "print()\n",
    "print('SVC')\n",
    "print(confusion_matrix(svc_predict))\n",
    "print()\n",
    "print('LinearSVC')\n",
    "print(confusion_matrix(linear_svc_predict, False))\n",
    "print()\n",
    "print('Random Forest')\n",
    "print(confusion_matrix(rf_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lower the better for `log_loss`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD: 0.37847249143305395\n",
      "Logistic Regression: 0.3731162798963883\n",
      "SVC: 0.4150758412082994\n",
      "Random Forest: 0.39579833325412017\n"
     ]
    }
   ],
   "source": [
    "print(f'SGD: {log_loss(sgd_predict)}')\n",
    "print(f'Logistic Regression: {log_loss(lr_predict)}')\n",
    "print(f'SVC: {log_loss(svc_predict)}')\n",
    "print(f'Random Forest: {log_loss(rf_predict)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The higher the better for `roc_auc_score`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD: 0.9158955308961009\n",
      "Logistic Regression: 0.9168170935623623\n",
      "SVC: 0.8879588812039219\n",
      "Random Forest: 0.9088888044387019\n"
     ]
    }
   ],
   "source": [
    "print(f'SGD: {roc_auc_score(sgd_predict)}')\n",
    "print(f'Logistic Regression: {roc_auc_score(lr_predict)}')\n",
    "print(f'SVC: {roc_auc_score(svc_predict)}')\n",
    "print(f'Random Forest: {roc_auc_score(rf_predict)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: We cannot compute `log_loss` or `roc_auc_score` for `LinearSVC` because it does not provide probability estimates.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance on sample tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy: [('neg', 0.0), ('neg', 0.0), ('neg', 0.0), ('neg', 0.0), ('pos', 1.0), ('neg', 0.0), ('pos', 1.0)]\n",
      "LogisticRegression: [('pos', 0.59993632091708504), ('pos', 0.80545738462427463), ('pos', 0.90904984569308278), ('pos', 0.80545738462427463), ('pos', 0.85428416471513524), ('pos', 0.80545738462427463), ('pos', 0.82428633915248184)]\n",
      "SVC: [('pos', 0.79790817731030483), ('pos', 0.74224643977964966), ('pos', 0.85309553545012395), ('pos', 0.74224643977964966), ('pos', 0.82893669789024926), ('pos', 0.74224643977964966), ('pos', 0.80199263468059634)]\n",
      "LinearSVC: ['pos', 'pos', 'pos', 'pos', 'pos', 'pos', 'pos']\n",
      "SGD: [('pos', 0.59646047671669478), ('pos', 0.85159259765683115), ('pos', 0.93425820756426969), ('pos', 0.85159259765683115), ('pos', 0.90840726546647588), ('pos', 0.85159259765683115), ('pos', 0.86340530996514875)]\n",
      "Random Forest: [('pos', 0.5399701926070386), ('pos', 0.92400000000000004), ('pos', 0.84232587787459112), ('pos', 0.92400000000000004), ('pos', 0.8786342592592592), ('pos', 0.92400000000000004), ('pos', 0.8998685446009389)]\n"
     ]
    }
   ],
   "source": [
    "sample_tasks = [\"Mow lawn\", \"Mow the lawn\", \"Buy new shoes\", \"Feed the dog\", \"Send report to Kyle\", \"Send the report to Kyle\", \"Peel the potatoes\"]\n",
    "features = [featurize(nlp(task)) for task in sample_tasks]\n",
    "\n",
    "tasks_dummy = [(l, p.prob('pos')*1.0) for l, p in zip(dummy.classify_many(features), dummy.prob_classify_many(features))]\n",
    "tasks_logistic = [(l, p.prob('pos')) for l,p in zip(logistic_regression.classify_many(features), logistic_regression.prob_classify_many(features))]\n",
    "tasks_svc = [(l, p.prob('pos')) for l,p in zip(svc.classify_many(features), svc.prob_classify_many(features))]\n",
    "tasks_linear_svc = linear_svc.classify_many(features)\n",
    "tasks_sgd = [(l, p.prob('pos')) for l,p in zip(sgd.classify_many(features), sgd.prob_classify_many(features))]\n",
    "tasks_rf = [(l, p.prob('pos')) for l,p in zip(random_forest.classify_many(features), random_forest.prob_classify_many(features))]\n",
    "\n",
    "print(f'Dummy: {tasks_dummy}')\n",
    "print(f'LogisticRegression: {tasks_logistic}')\n",
    "print(f'SVC: {tasks_svc}')\n",
    "print(f'LinearSVC: {tasks_linear_svc}')\n",
    "print(f'SGD: {tasks_sgd}')\n",
    "print(f'Random Forest: {tasks_rf}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: `LinearSVC` is not implemented to provide probability estimates (https://github.com/scikit-learn/scikit-learn/issues/4820)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps and Improvements\n",
    "\n",
    "1. Training set may be too specific/not relevant enough (recipe instructions for positive dataset, recipe descriptions+short movie reviews for negative dataset)\n",
    "2. Throwing features into a blender - need to understand value of each\n",
    "    - What feature \"classes\" tend to perform the best/worst?\n",
    "    - [PCA](http://jotterbach.github.io/2016/03/24/Principal_Component_Analysis/): Reducing dimensionality using most informative feature information\n",
    "3. Phrase vectorizations of all 0s - how problematic is this?\n",
    "4. Varying feature vector lengths - does this matter?\n",
    "5. Voting - POS taggers and classifiers\n",
    "    - Ensembles (http://scikit-learn.org/stable/modules/ensemble.html)\n",
    "6. Combining verb phrases\n",
    "7. Cross validation, grid search\n",
    "8. Look at examples from different quadrants of the confusion matrix - is there something we can learn?\n",
    "    - Same idea with the classification report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Things abandoned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I needed a library that supports dependency parsing, which NLTK does not... so I thought I'd add the [Stanford CoreNLP](https://stanfordnlp.github.io/CoreNLP/) toolkit and [its associated software](https://nlp.stanford.edu/software/) to NLTK. However, there are many conflicting instructions for installing the Java-based project, depending on NLTK version used. By the time I figured this out, the installation had become a time sink. So I abandoned this effort in favor of Spacy.io.\n",
    "\n",
    "I might return this way if I want to improve results/implement a voter system between the various linguistic and classification methods later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = [s for l in lines for s in sent_tokenize(l)] # punkt\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagged_sentences = []\n",
    "for s in sentences:\n",
    "    words = word_tokenize(s)\n",
    "    tagged = nltk.pos_tag(words) # averaged_perceptron_tagger\n",
    "    tagged_sentences.append(tagged)\n",
    "print(tagged_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: POS accuracy\n",
    "\n",
    "`Run down to the shop, will you, Peter` is parsed unexpectedly by `nltk.pos_tag`:\n",
    "> `[('Run', 'NNP'), ('down', 'RB'), ('to', 'TO'), ('the', 'DT'), ('shop', 'NN'), (',', ','), ('will', 'MD'), ('you', 'PRP'), (',', ','), ('Peter', 'NNP')]`\n",
    "\n",
    "`Run` is tagged as a `NNP (proper noun, singular)`\n",
    "\n",
    "I expected an output more like what the [Stanford Parser](http://nlp.stanford.edu:8080/parser/) provides:\n",
    "> `Run/VBG down/RP to/TO the/DT shop/NN ,/, will/MD you/PRP ,/, Peter/NNP`\n",
    "\n",
    "`Run` is tagged as a `VGB (verb, gerund/present participle)` - still not quite the `VB` I want, but at least it's a `V*`\n",
    "\n",
    "_MEANWHILE..._\n",
    "\n",
    "`nltk.pos_tag` did better with:\n",
    "> `[('Do', 'VB'), ('not', 'RB'), ('clean', 'VB'), ('soot', 'NN'), ('off', 'IN'), ('the', 'DT'), ('window', 'NN')]`\n",
    "\n",
    "Compared to [Stanford CoreNLP](http://nlp.stanford.edu:8080/corenlp/process) (note that this is different than what [Stanford Parser](http://nlp.stanford.edu:8080/parser/) outputs):\n",
    "> `(ROOT (S (VP (VB Do) (NP (RB not) (JJ clean) (NN soot)) (PP (IN off) (NP (DT the) (NN window))))))`\n",
    "\n",
    "Concern: _clean_ as `VB (verb, base form)` vs `JJ (adjective)` \n",
    "\n",
    "**IMPROVE** POS taggers should vote: nltk.pos_tag (averaged_perceptron_tagger), Stanford Parser, CoreNLP, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note what Spacy POS tagger did with `Run down to the shop, will you Peter`:\n",
    "\n",
    "`Run/VB down/RP to/IN the shop/NN ,/, will/MD you/PRP ,/, Peter/NNP`\n",
    "\n",
    "    where `Run` is the `VB` I expected from POS tagging (compared to `nltk.pos_tag` result of `NNP`). Also note that Spacy collapses `the shop` into a single unit, which should be helpful during featurization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Featurization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "featuresets = []\n",
    "for ts in tagged_sentences:\n",
    "    s_features = defaultdict(int)\n",
    "    for idx, tup in enumerate(ts):\n",
    "        #print(tup)\n",
    "        pos = tup[1]\n",
    "        # FeatureName.VERB\n",
    "        is_verb = re.match(r'VB.?', pos) is not None\n",
    "        print(tup, is_verb)\n",
    "        if is_verb:\n",
    "            s_features[FeatureName.VERB] += 1\n",
    "            # FOLLOWING_POS\n",
    "            next_idx = idx + 1;\n",
    "            if next_idx < len(ts):\n",
    "                s_features[f'{FeatureName.FOLLOWING}_{ts[next_idx][1]}'] += 1\n",
    "            # VERB_MODIFIER\n",
    "            # VERB_MODIFYING\n",
    "        else:\n",
    "            s_features[FeatureName.VERB] = 0\n",
    "    featuresets.append(dict(s_features))\n",
    "\n",
    "print()\n",
    "print(featuresets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Stanford NLP](https://nlp.stanford.edu/software/)\n",
    "Setup guide used: https://stackoverflow.com/a/34112695"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get dependency parser, NER, POS tagger\n",
    "!wget https://nlp.stanford.edu/software/stanford-parser-full-2017-06-09.zip\n",
    "!wget https://nlp.stanford.edu/software/stanford-ner-2017-06-09.zip\n",
    "!wget https://nlp.stanford.edu/software/stanford-postagger-full-2017-06-09.zip\n",
    "!unzip stanford-parser-full-2017-06-09.zip\n",
    "!unzip stanford-ner-2017-06-09.zip\n",
    "!unzip stanford-postagger-full-2017-06-09.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.parse.stanford import StanfordParser\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "from nltk.parse.stanford import StanfordNeuralDependencyParser\n",
    "from nltk.tag.stanford import StanfordPOSTagger, StanfordNERTagger\n",
    "from nltk.tokenize.stanford import StanfordTokenizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
