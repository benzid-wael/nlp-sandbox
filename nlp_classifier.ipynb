{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP for Task Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hypothesis**: Part of Speech (POS) tagging and syntactic dependency parsing provides valuable information for classifying imperative phrases. The thinking is that being able to detect imperative phrases will transfer well to detecting tasks and to-dos.\n",
    "\n",
    "#### Some Terminology\n",
    "- [_Imperative mood_](https://en.wikipedia.org/wiki/Imperative_mood) is \"used principally for ordering, requesting or advising the listener to do (or not to do) something... also often used for giving instructions as to how to perform a task.\"\n",
    "- _Part of speech (POS)_ is a way of categorizing a word based on its syntactic function.\n",
    "    - The POS tagger from Spacy.io that is used in this notebook differentiates between [*pos_* and *tag_*](https://spacy.io/docs/api/annotation#pos-tagging-english) - *POS (pos_)* refers to \"coarse-grained part-of-speech\" like `VERB`, `ADJ`, or `PUNCT`; and *POSTAG (tag_)* refers to \"fine-grained part-of-speech\" like `VB`, `JJ`, or `.`.\n",
    "- _Syntactic dependency parsing_ is a way of connecting words based on syntactic relationships, [such as](https://spacy.io/docs/api/annotation#dependency-parsing-english) `DOBJ` (direct object), `PREP` (prepositional modifier), or `POBJ` (object of preposition).\n",
    "    - Check out the dependency parse of the phrase [\"Send the report to Kyle by tomorrow\"](https://demos.explosion.ai/displacy/?text=Send%20the%20report%20to%20Kyle%20by%20tomorrow&model=en&cpu=1&cph=1) as an example.\n",
    "\n",
    "### Proposed Features\n",
    "The imperative mood centers around _actions_, and actions are generally represented in English using verbs. So the features are engineered to also center on the VERB:\n",
    "1. `FeatureName.VERB`: Does the phrase contain `VERB`(s) of the tag form `VB*`?\n",
    "2. `FeatureName.FOLLOWING_POS`: Are the words following the `VERB`(s) of certain parts of speech?\n",
    "3. `FeatureName.FOLLOWING_POSTAG`: Are the words following the `VERB`(s) of certain POS tags?\n",
    "4. `FeatureName.CHILD_DEP`: Are the `VERB`(s) parents of certain syntactic dependencies?\n",
    "5. `FeatureName.PARENT_DEP`: Are the `VERB`(s) children of certain syntactic dependencies?\n",
    "6. `FeatureName.CHILD_POS`: Are the syntactic dependencies that the `VERB`(s) are children of of certain parts of speech?\n",
    "7. `FeatureName.CHILD_POSTAG`: Are the syntactic dependencies that the `VERB`(s) are children of of certain POS tags?\n",
    "8. `FeatureName.PARENT_POS`: Are the syntactic dependencies that the `VERB`(s) parent of certain parts of speech?\n",
    "9. `FeatureName.PARENT_POSTAG`: Are the syntactic dependencies that the `VERB`(s) parent of certain POS tags?\n",
    "\n",
    "**Notes:**\n",
    "- Features 2-9 all depend on feature 1 between `True`; if `False`, phrase vectorization will result in all zeroes.\n",
    "- When features 2-9 are applied to actual phrases, they will append identifying informating about the feature in the form of `_*` (e.g., `FeatureName.FOLLOWING_POSTAG_WRB`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a recipe corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wrote and ran `epicurious_recipes.py`\\* to scrape Epicurious.com for recipe instructions and descriptions. I then performed some manual cleanup of the script results. Output is in `epicurious-pos.txt` and `epicurious-neg.txt`.\n",
    "\n",
    "\\* _script (very) loosely based off of https://github.com/benosment/hrecipe-parse_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** that deriving all negative examples in the training set from Epicurious recipe descriptions would result in negative examples that are longer and syntactically more complicated than the positive examples. This is a form of bias.\n",
    "\n",
    "To (hopefully?) correct for this a bit, I will add the short movie reviews found at https://pythonprogramming.net/static/downloads/short_reviews/ as more negative examples.\n",
    "\n",
    "This still feels weird because we're selecting negative examples only from specific categories of text (recipe descriptions, short movie reviews) - just because they're readily available. Further, most positive examples are recipe instructions - also a specific (and not necessarily related to the main \"task\" category) category of text.\n",
    "\n",
    "Ultimately though, this recipe corpus is a **stopgap/proof of concept** for a corpus more relevant to tasks later on, so I won't worry further about this for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pandas import read_csv\n",
    "from numpy import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BASE_DIR = os.getcwd()\n",
    "data_path = BASE_DIR + '/data.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Be kind</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Get out of here</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Look this over</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Paul, do your homework now</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Do not clean soot off the window</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Text Label\n",
       "0                           Be kind   pos\n",
       "1                   Get out of here   pos\n",
       "2                    Look this over   pos\n",
       "3        Paul, do your homework now   pos\n",
       "4  Do not clean soot off the window   pos"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = read_csv(data_path, sep='\\t', header=None, names=['Text', 'Label'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_data_split = list(df.loc[df.Label == 'pos'].Text)\n",
    "neg_data_split = list(df.loc[df.Label == 'neg'].Text)\n",
    "\n",
    "num_pos = len(pos_data_split)\n",
    "num_neg = len(neg_data_split)\n",
    "\n",
    "# 50/50 split between the number of positive and negative samples\n",
    "num_per_class = num_pos if num_pos < num_neg else num_neg\n",
    "\n",
    "# shuffle samples\n",
    "random.shuffle(pos_data_split)\n",
    "random.shuffle(neg_data_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines = []\n",
    "for l in pos_data_split[:num_per_class]:\n",
    "    lines.append((l, 'pos'))\n",
    "for l in neg_data_split[:num_per_class]:\n",
    "    lines.append((l, 'neg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Features as defined in the introduction\n",
    "from enum import Enum, auto\n",
    "class FeatureName(Enum):\n",
    "    VERB = auto()\n",
    "    FOLLOWING_POS = auto()\n",
    "    FOLLOWING_POSTAG = auto()\n",
    "    CHILD_DEP = auto()\n",
    "    PARENT_DEP = auto()\n",
    "    CHILD_POS = auto()\n",
    "    CHILD_POSTAG = auto()\n",
    "    PARENT_POS = auto()\n",
    "    PARENT_POSTAG = auto()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [spaCy.io](https://spacy.io/) for NLP\n",
    "_Because Stanford CoreNLP is hard to install for Python_\n",
    "\n",
    "Found Spacy through an article on [\"Training a Classifier for Relation Extraction from Medical Literature\"](https://www.microsoft.com/developerblog/2016/09/13/training-a-classifier-for-relation-extraction-from-medical-literature/) ([GitHub](https://github.com/CatalystCode/corpus-to-graph-ml))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"nltk_library_comparison.png\" alt=\"NLTK library comparison chart https://spacy.io/docs/api/#comparison\" style=\"width: 400px; margin: 0;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!conda config --add channels conda-forge\n",
    "#!conda install spacy\n",
    "#!python -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Spacy Data Model for NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy's sentence segmentation is lacking... https://github.com/explosion/spaCy/issues/235. So each '\\n' will start a new Spacy Doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_spacy_docs(ll):\n",
    "    dd = [(nlp(l[0]), l[1]) for l in ll]\n",
    "    # collapse noun phrases into single compounds\n",
    "    for d in dd:\n",
    "        for np in d[0].noun_chunks:\n",
    "            np.merge(np.root.tag_, np.text, np.root.ent_type_)\n",
    "    return dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs = create_spacy_docs(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization, POS tagging, and dependency parsing happened automatically with the `nlp(line)` calls above! So let's look at the outputs.\n",
    "\n",
    "https://spacy.io/docs/usage/data-model and https://spacy.io/docs/api/doc will be useful going forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Spread on a rimmed baking sheet and roast until an instant-read thermometer inserted into thickest part of thigh registers 165°F, 15–17 minutes.]\n",
      "[Thread 2 pieces of chicken onto each skewer.]\n",
      "[Peel off the top layer of cling film then invert the pastry circle into the prepared tart tin, gently pressing it into all the edges.]\n",
      "[Remove the thin clear sliver of cartilage; discard.]\n",
      "[Butter a 9x9-inch cake pan.]\n",
      "[Serve with a crunchy green salad.]\n",
      "[Let steep 4 minutes. Discard tea bags and let cool slightly (you should have about 3 1/2 cups tea).]\n",
      "[Serve with sauces alongside, if desired.]\n",
      "[Transfer chickens to a cutting board and let rest at least 10 minutes before carving.]\n",
      "[Blend lemonade ice cubes, 1/2 cup vodka, and 1/2 cup lemonade in a blender until smooth.]\n"
     ]
    }
   ],
   "source": [
    "for doc in docs[:10]:\n",
    "    print(list(doc[0].sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[a rimmed baking sheet, roast, an instant-read thermometer, thickest part, thigh registers]\n",
      "[Thread 2 pieces, chicken, each skewer]\n",
      "[Peel, the top layer, film, the pastry circle, the prepared tart tin, it, all the edges]\n",
      "[the thin clear sliver, cartilage]\n",
      "[a 9x9-inch cake pan]\n",
      "[a crunchy green salad]\n",
      "[tea bags, you, about 3 1/2 cups tea]\n",
      "[sauces]\n",
      "[Transfer chickens, a cutting board]\n",
      "[lemonade ice cubes, 1/2 cup vodka, 1/2 cup lemonade, a blender]\n"
     ]
    }
   ],
   "source": [
    "for doc in docs[:10]:\n",
    "    print(list(doc[0].noun_chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Spacy's dependency graph visualization](https://demos.explosion.ai/displacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spread ROOT spread VERB VB Spread [on, until, .]\n",
      "on prep on ADP IN Spread [a rimmed baking sheet]\n",
      "a rimmed baking sheet pobj a rimmed baking sheet NOUN NN on [and, roast]\n",
      "and cc and CCONJ CC a rimmed baking sheet []\n",
      "roast conj roast NOUN NN a rimmed baking sheet []\n",
      "until prep until ADP IN Spread [an instant-read thermometer]\n",
      "an instant-read thermometer pobj an instant-read thermometer NOUN NN until [inserted]\n",
      "inserted acl insert VERB VBN an instant-read thermometer [into, minutes]\n",
      "into prep into ADP IN inserted [thickest part]\n",
      "thickest part pobj thickest part NOUN NN into [of]\n",
      "of prep of ADP IN thickest part [thigh registers]\n",
      "thigh registers pobj thigh registers NOUN NNS of [165°F]\n",
      "165°F appos 165°f NUM CD thigh registers [,]\n",
      ", punct , PUNCT , 165°F []\n",
      "15–17 nummod 15–17 NUM CD minutes []\n",
      "minutes npadvmod minute NOUN NNS inserted [15–17]\n",
      ". punct . PUNCT . Spread []\n",
      "Thread 2 pieces ROOT Thread 2 pieces NOUN NNS Thread 2 pieces [of, onto, .]\n",
      "of prep of ADP IN Thread 2 pieces [chicken]\n",
      "chicken pobj chicken NOUN NN of []\n",
      "onto prep onto ADP IN Thread 2 pieces [each skewer]\n",
      "each skewer pobj each skewer NOUN NN onto []\n",
      ". punct . PUNCT . Thread 2 pieces []\n",
      "Peel ROOT Peel PROPN NNP Peel [off, invert]\n",
      "off prep off ADP IN Peel [the top layer]\n",
      "the top layer pobj the top layer NOUN NN off [of]\n",
      "of prep of ADP IN the top layer [cling]\n",
      "cling pcomp cl VERB VBG of [film]\n",
      "film dobj film NOUN NN cling []\n",
      "then advmod then ADV RB invert []\n",
      "invert dep invert VERB VB Peel [then, the pastry circle, into, pressing, .]\n",
      "the pastry circle dobj the pastry circle NOUN NN invert []\n",
      "into prep into ADP IN invert [the prepared tart tin]\n",
      "the prepared tart tin pobj the prepared tart tin NOUN NN into [,]\n",
      ", punct , PUNCT , the prepared tart tin []\n",
      "gently advmod gently ADV RB pressing []\n",
      "pressing advcl press VERB VBG invert [gently, it, into]\n",
      "it dobj it PRON PRP pressing []\n",
      "into prep into ADP IN pressing [all the edges]\n",
      "all the edges pobj all the edges NOUN NNS into []\n",
      ". punct . PUNCT . invert []\n",
      "Remove ROOT remove VERB VB Remove [discard, .]\n",
      "the thin clear sliver nsubj the thin clear sliver NOUN NN discard [of]\n",
      "of prep of ADP IN the thin clear sliver [cartilage]\n",
      "cartilage pobj cartilage NOUN NN of [;]\n",
      "; punct ; PUNCT , cartilage []\n",
      "discard ccomp discard NOUN NN Remove [the thin clear sliver]\n",
      ". punct . PUNCT . Remove []\n",
      "Butter ROOT butter VERB VB Butter [a 9x9-inch cake pan, .]\n",
      "a 9x9-inch cake pan dobj a 9x9-inch cake pan NOUN NN Butter []\n",
      ". punct . PUNCT . Butter []\n"
     ]
    }
   ],
   "source": [
    "for doc in docs[:5]:\n",
    "    for token in doc[0]:\n",
    "        print(token.text, token.dep_, token.lemma_, token.pos_, token.tag_, token.head, list(token.children))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Featurization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def featurize(d):\n",
    "    s_features = defaultdict(int)\n",
    "    for idx, token in enumerate(d):\n",
    "        if re.match(r'VB.?', token.tag_) is not None: # note: not using token.pos == VERB because this also includes BES, HVS, MD tags \n",
    "            s_features[FeatureName.VERB.name] += 1\n",
    "            # FOLLOWING_POS\n",
    "            # FOLLOWING_POSTAG\n",
    "            next_idx = idx + 1;\n",
    "            if next_idx < len(d):\n",
    "                s_features[f'{FeatureName.FOLLOWING_POS.name}_{d[next_idx].pos_}'] += 1\n",
    "                s_features[f'{FeatureName.FOLLOWING_POSTAG.name}_{d[next_idx].tag_}'] += 1\n",
    "            # PARENT_DEP\n",
    "            # PARENT_POS\n",
    "            # PARENT_POSTAG\n",
    "            '''\n",
    "            \"Because the syntactic relations form a tree, every word has exactly one head.\n",
    "            You can therefore iterate over the arcs in the tree by iterating over the words in the sentence.\"\n",
    "            https://spacy.io/docs/usage/dependency-parse#navigating\n",
    "            '''\n",
    "            if (token.head is not token):\n",
    "                s_features[f'{FeatureName.PARENT_DEP.name}_{token.head.dep_.upper()}'] += 1\n",
    "                s_features[f'{FeatureName.PARENT_POS.name}_{token.head.pos_}'] += 1\n",
    "                s_features[f'{FeatureName.PARENT_POSTAG.name}_{token.head.tag_}'] += 1\n",
    "            # CHILD_DEP\n",
    "            # CHILD_POS\n",
    "            # CHILD_POSTAG\n",
    "            for child in token.children:\n",
    "                s_features[f'{FeatureName.CHILD_DEP.name}_{child.dep_.upper()}'] += 1\n",
    "                s_features[f'{FeatureName.CHILD_POS.name}_{child.pos_}'] += 1\n",
    "                s_features[f'{FeatureName.CHILD_POSTAG.name}_{child.tag_}'] += 1\n",
    "    return dict(s_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featuresets = [(doc[0], (featurize(doc[0]), doc[1])) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats on number of features per example:\n",
      "mean: 23.039031836022676\n",
      "stdev: 14.543193390809183\n",
      "median: 23.0\n",
      "mode: 0\n",
      "max: 75\n",
      "min: 0\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean, median, mode, stdev\n",
    "f_lengths = [len(fs[1][0]) for fs in featuresets]\n",
    "\n",
    "print('Stats on number of features per example:')\n",
    "print(f'mean: {mean(f_lengths)}')\n",
    "print(f'stdev: {stdev(f_lengths)}')\n",
    "print(f'median: {median(f_lengths)}')\n",
    "print(f'mode: {mode(f_lengths)}')\n",
    "print(f'max: {max(f_lengths)}')\n",
    "print(f'min: {min(f_lengths)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Spread on a rimmed baking sheet and roast until an instant-read thermometer inserted into thickest part of thigh registers 165°F, 15–17 minutes.,\n",
       "  ({'CHILD_DEP_NPADVMOD': 1,\n",
       "    'CHILD_DEP_PREP': 3,\n",
       "    'CHILD_DEP_PUNCT': 1,\n",
       "    'CHILD_POSTAG_.': 1,\n",
       "    'CHILD_POSTAG_IN': 3,\n",
       "    'CHILD_POSTAG_NNS': 1,\n",
       "    'CHILD_POS_ADP': 3,\n",
       "    'CHILD_POS_NOUN': 1,\n",
       "    'CHILD_POS_PUNCT': 1,\n",
       "    'FOLLOWING_POSTAG_IN': 2,\n",
       "    'FOLLOWING_POS_ADP': 2,\n",
       "    'PARENT_DEP_POBJ': 1,\n",
       "    'PARENT_POSTAG_NN': 1,\n",
       "    'PARENT_POS_NOUN': 1,\n",
       "    'VERB': 2},\n",
       "   'pos')),\n",
       " (Thread 2 pieces of chicken onto each skewer., ({}, 'pos'))]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featuresets[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On one run, the above line printed the following featureset:\n",
    "`(Gather foil loosely on top and bake for 1 1/2 hours., ({}, 'pos'))`\n",
    "\n",
    "This is because the Spacy.io POS tagger provided this:\n",
    "   `Gather/NNP foil/NN loosely/RB on/IN top/NN and/CC bake/NN for/IN 1 1/2 hours./NNS`\n",
    "\n",
    "...with no VERBs tagged, which is incorrect.\n",
    "\n",
    "\"Voting - POS taggers and classifiers\" in the _Next Steps/Improvements_ section below is meant to improve on this.\n",
    "\n",
    "---\n",
    "Compare to [Stanford CoreNLP POS tagger](http://nlp.stanford.edu:8080/corenlp/process):\n",
    "   `Gather/VB foil/NN loosely/RB on/IN top/JJ and/CC bake/VB for/IN 1 1/2/CD hours/NNS ./.`\n",
    "\n",
    "And [Stanford Parser](http://nlp.stanford.edu:8080/parser/index.jsp):\n",
    "   `Gather/NNP foil/VB loosely/RB on/IN top/NN and/CC bake/VB for/IN 1 1/2/CD hours/NNS ./.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training samples: 3669\n",
      "# test samples: 917\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(featuresets)\n",
    "\n",
    "num_classes = 2\n",
    "split_num = round(num_per_class*num_classes / 5)\n",
    "\n",
    "# train and test sets\n",
    "testing_set = [fs[1] for i, fs in enumerate(featuresets[:split_num])]\n",
    "training_set =  [fs[1] for i, fs in enumerate(featuresets[split_num:])]\n",
    "\n",
    "print(f'# training samples: {len(training_set)}')\n",
    "print(f'# test samples: {len(testing_set)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# decoupling the functionality of nltk.classify.accuracy\n",
    "def predict(classifier, gold, prob=True):\n",
    "    if (prob is True):\n",
    "        predictions = classifier.prob_classify_many([fs for (fs, ll) in gold])\n",
    "    else:\n",
    "        predictions = classifier.classify_many([fs for (fs, ll) in gold])\n",
    "    return list(zip(predictions, [ll for (fs, ll) in gold]))\n",
    "\n",
    "def accuracy(predicts, prob=True):\n",
    "    if (prob is True):\n",
    "        correct = [label == prediction.max() for (prediction, label) in predicts]\n",
    "    else:\n",
    "        correct = [label == prediction for (prediction, label) in predicts]\n",
    "        \n",
    "    if correct:\n",
    "        return sum(correct) / len(correct)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note below the use of `DummyClassifier` to provide a simple sanity check, a baseline of random predictions. `stratified` means it \"generates random predictions by respecting the training set class distribution.\" (http://scikit-learn.org/stable/modules/model_evaluation.html#dummy-estimators)\n",
    "\n",
    "> More generally, when the accuracy of a classifier is too close to random, it probably means that something went wrong: features are not helpful, a hyperparameter is not correctly tuned, the classifier is suffering from class imbalance, etc…\n",
    "\n",
    "If a classifier can beat the `DummyClassifier`, it is at least learning something valuable! How valuable is another question..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy classifier accuracy percent: 50.4907306434024\n",
      "NaiveBayes classifier accuracy percent: 67.2846237731734\n",
      "MultinomialNB classifier accuracy percent: 79.93456924754635\n",
      "BernoulliNB classifier accuracy percent: 73.93675027262813\n",
      "LogisticRegression classifier accuracy percent: 83.75136314067612\n",
      "SGD classifier accuracy percent: 77.09923664122137\n",
      "SVC classifier accuracy percent: 81.02508178844057\n",
      "LinearSVC classifier accuracy percent: 83.6423118865867\n",
      "DecisionTree classifier accuracy percent: 77.09923664122137\n",
      "RandomForest classifier accuracy percent: 82.33369683751364\n"
     ]
    }
   ],
   "source": [
    "from nltk import NaiveBayesClassifier\n",
    "from nltk.classify.decisiontree import DecisionTreeClassifier\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "dummy = SklearnClassifier(DummyClassifier(strategy='stratified', random_state=0))\n",
    "dummy.train(training_set)\n",
    "dummy_predict = predict(dummy, testing_set)\n",
    "dummy_accuracy = accuracy(dummy_predict)\n",
    "print(\"Dummy classifier accuracy percent:\", dummy_accuracy*100)\n",
    "\n",
    "nb = NaiveBayesClassifier.train(training_set)\n",
    "nb_predict = predict(nb, testing_set)\n",
    "nb_accuracy = accuracy(nb_predict)\n",
    "print(\"NaiveBayes classifier accuracy percent:\", nb_accuracy*100)\n",
    "\n",
    "multinomial_nb = SklearnClassifier(MultinomialNB())\n",
    "multinomial_nb.train(training_set)\n",
    "mnb_predict = predict(multinomial_nb, testing_set)\n",
    "mnb_accuracy = accuracy(mnb_predict)\n",
    "print(\"MultinomialNB classifier accuracy percent:\", mnb_accuracy*100)\n",
    "\n",
    "bernoulli_nb = SklearnClassifier(BernoulliNB())\n",
    "bernoulli_nb.train(training_set)\n",
    "bnb_predict = predict(bernoulli_nb, testing_set)\n",
    "bnb_accuracy = accuracy(bnb_predict)\n",
    "print(\"BernoulliNB classifier accuracy percent:\", bnb_accuracy*100)\n",
    "\n",
    "# ??logistic_regression._clf\n",
    "#   sklearn.svm.LinearSVC : learns SVM models using the same algorithm.\n",
    "logistic_regression = SklearnClassifier(LogisticRegression())\n",
    "logistic_regression.train(training_set)\n",
    "lr_predict = predict(logistic_regression, testing_set)\n",
    "lr_accuracy = accuracy(lr_predict)\n",
    "print(\"LogisticRegression classifier accuracy percent:\", lr_accuracy*100)\n",
    "\n",
    "# ??sgd._clf\n",
    "#    The 'log' loss gives logistic regression, a probabilistic classifier.\n",
    "# ??linear_svc._clf\n",
    "#   can optimize the same cost function as LinearSVC\n",
    "#   by adjusting the penalty and loss parameters. In addition it requires\n",
    "#   less memory, allows incremental (online) learning, and implements\n",
    "#   various loss functions and regularization regimes.\n",
    "sgd = SklearnClassifier(SGDClassifier(loss='log'))\n",
    "sgd.train(training_set)\n",
    "sgd_predict = predict(sgd, testing_set)\n",
    "sgd_accuracy = accuracy(sgd_predict)\n",
    "print(\"SGD classifier accuracy percent:\", sgd_accuracy*100)\n",
    "\n",
    "# slow\n",
    "# using libsvm with kernel 'rbf' (radial basis function)\n",
    "svc = SklearnClassifier(SVC(probability=True))\n",
    "svc.train(training_set)\n",
    "svc_predict = predict(svc, testing_set)\n",
    "svc_accuracy = accuracy(svc_predict)\n",
    "print(\"SVC classifier accuracy percent:\", svc_accuracy*100)\n",
    "\n",
    "# ??linear_svc._clf\n",
    "#    Similar to SVC with parameter kernel='linear', but implemented in terms of\n",
    "#    liblinear rather than libsvm, so it has more flexibility in the choice of\n",
    "#    penalties and loss functions and should scale better to large numbers of\n",
    "#    samples.\n",
    "#    Prefer dual=False when n_samples > n_features.\n",
    "# Using CalibratedClassifierCV as wrapper to get predict probabilities (https://stackoverflow.com/a/39712590)\n",
    "linear_svc = SklearnClassifier(CalibratedClassifierCV(LinearSVC(dual=False)))\n",
    "linear_svc.train(training_set)\n",
    "linear_svc_predict = predict(linear_svc, testing_set)\n",
    "linear_svc_accuracy = accuracy(linear_svc_predict)\n",
    "print(\"LinearSVC classifier accuracy percent:\", linear_svc_accuracy*100)\n",
    "\n",
    "# slower\n",
    "dt = DecisionTreeClassifier.train(training_set)\n",
    "dt_predict = predict(dt, testing_set, False)\n",
    "dt_accuracy = accuracy(dt_predict, False)\n",
    "print(\"DecisionTree classifier accuracy percent:\", dt_accuracy*100)\n",
    "\n",
    "random_forest = SklearnClassifier(RandomForestClassifier(n_estimators = 100))\n",
    "random_forest.train(training_set)\n",
    "rf_predict = predict(random_forest, testing_set)\n",
    "rf_accuracy = accuracy(rf_predict)\n",
    "print(\"RandomForest classifier accuracy percent:\", rf_accuracy*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD: Multiple Epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sgd` classifiers improves with epochs. `??sgd._clf` tells us that the default number of epochs `n_iter` is 5. So let's run more epochs. Also not that the training_set shuffle is `True` by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier classifier accuracy percent (epochs: 1000): 83.53326063249727\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "sgd = SklearnClassifier(SGDClassifier(loss='log', n_iter=num_epochs))\n",
    "sgd.train(training_set)\n",
    "sgd_predict = predict(sgd, testing_set)\n",
    "sgd_accuracy = accuracy(sgd_predict)\n",
    "print(f\"SGDClassifier classifier accuracy percent (epochs: {num_epochs}):\", sgd_accuracy*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, 1000 epochs run very quickly! And `SGDClassifier` performance has improved with more iterations.\n",
    "\n",
    "*Also note that we can set `warm_start` to `True` if we want to take advantage of online learning and reuse the solution of the previous call.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VotingClassifier\n",
    "\n",
    "We're going to create an ensemble classifier by letting our top-performing classifiers, which consistently perform with >80% accuracy &mdash; LogisticRegression, SVC, LinearSVC, SGD, and RandomForest &mdash; vote on each prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soft voting classifier accuracy percent: 83.96946564885496\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "voting = SklearnClassifier(VotingClassifier(estimators=[\n",
    "    ('lr', logistic_regression._clf),\n",
    "    ('linear_svc', linear_svc._clf),\n",
    "    ('sgd', sgd._clf),\n",
    "    ('rf', random_forest._clf)\n",
    "], voting='soft', n_jobs=-1))\n",
    "voting.train(training_set)\n",
    "voting_predict = predict(voting, testing_set)\n",
    "voting_accuracy = accuracy(voting_predict)\n",
    "print(\"Soft voting classifier accuracy percent:\", voting_accuracy*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to scope analysis down to our top-performing classifiers, which consistently perform with >80% accuracy: `LogisticRegression`, `LinearSVC`, `SGD`, and `RandomForest` (excluding `SVC` due to its slowness).\n",
    "\n",
    "We'll also include `Dummy` as a baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most Informative Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/a/11140887\n",
    "def show_most_informative_features(vectorizer, clf, n=20):\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))\n",
    "    top = zip(coefs_with_fns[:round(n/2)], coefs_with_fns[:-(round(n/2) + 1):-1])\n",
    "    for (coef_1, fn_1), (coef_2, fn_2) in top:\n",
    "        print(\"\\t%.4f\\t%-15s\\t\\t%.4f\\t%-15s\" % (coef_1, fn_1, coef_2, fn_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD\n",
      "\t-2.5593\tCHILD_DEP_AGENT\t\t2.3789\tCHILD_DEP_ADVMOD||XCOMP\n",
      "\t-2.1129\tCHILD_DEP_INTJ \t\t1.8852\tCHILD_POSTAG_-RRB-\n",
      "\t-2.0969\tCHILD_POSTAG_``\t\t1.7146\tPARENT_DEP_DOBJ||XCOMP\n",
      "\t-2.0024\tCHILD_POSTAG_HYPH\t\t1.7028\tPARENT_POS_PROPN\n",
      "\t-1.9718\tFOLLOWING_POSTAG_WRB\t\t1.7028\tPARENT_POSTAG_NNP\n",
      "\t-1.9480\tCHILD_POSTAG_''\t\t1.6534\tFOLLOWING_POSTAG_RBS\n",
      "\t-1.8508\tPARENT_POSTAG_CD\t\t1.5757\tPARENT_DEP_DEP \n",
      "\t-1.8508\tPARENT_POS_NUM \t\t1.5756\tFOLLOWING_POS_NUM\n",
      "\n",
      "Logistic Regression\n",
      "\t-2.0960\tCHILD_DEP_AGENT\t\t1.5310\tCHILD_POSTAG_-RRB-\n",
      "\t-1.7950\tCHILD_POSTAG_HYPH\t\t1.4006\tCHILD_DEP_NPADVMOD\n",
      "\t-1.5986\tPARENT_POSTAG_VBZ\t\t1.3353\tPARENT_POS_PROPN\n",
      "\t-1.5775\tCHILD_POSTAG_''\t\t1.3353\tPARENT_POSTAG_NNP\n",
      "\t-1.4823\tCHILD_POSTAG_WDT\t\t1.2909\tFOLLOWING_POS_NUM\n",
      "\t-1.4100\tCHILD_DEP_NEG  \t\t1.2909\tFOLLOWING_POSTAG_CD\n",
      "\t-1.3872\tCHILD_POSTAG_WP\t\t1.0908\tPARENT_POSTAG_VB\n",
      "\t-1.3437\tCHILD_DEP_INTJ \t\t1.0876\tCHILD_POSTAG_-LRB-\n"
     ]
    }
   ],
   "source": [
    "print('SGD')\n",
    "show_most_informative_features(sgd._vectorizer, sgd._clf, 15)\n",
    "print()\n",
    "print('Logistic Regression')\n",
    "show_most_informative_features(logistic_regression._vectorizer, logistic_regression._clf, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: Because `CalibratedClassifierCV` has no attribute `coef_`, we cannot show the most informative features for `LinearSVC` while it's wrapped. `Random Forest` also lacks `coef_`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'adjective, superlative'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"JJS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Negative coefficients**:\n",
    "- VERB parents [`AGENT`](http://universaldependencies.org/docs/sv/dep/nmod-agent.html): \"used for agents of passive verbs\" - interpreting this to mean that _existence of passive verbs (i.e., the opposite of active verbs) means negative correlation with it being imperative_\n",
    "- VERB followed by a `WRB`: \"wh-adverb\" (where, when)\n",
    "- VERB is a child of [`AMOD`](http://universaldependencies.org/en/dep/amod.html): \"any adjective or adjectival phrase that serves to modify the meaning\" of the verb\n",
    "\n",
    "\n",
    "**Positive coefficients**:\n",
    "- VERB parents a `-RRB-`: \"right round bracket\"\n",
    "- VERB is a child of `PROPN`: \"proper noun\"\n",
    "- VERB is a child of `NNP`: \"noun, proper singular\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Random Forest` has another built-in way of determining \"feature importance\".\n",
    "\n",
    "**TODO**: How to maps feature #s to feature names?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEICAYAAABF82P+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x185a750ec18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. feature 33 (0.104534)\n",
      "2. feature 247 (0.046676)\n",
      "3. feature 101 (0.035090)\n",
      "4. feature 242 (0.033077)\n",
      "5. feature 110 (0.025223)\n",
      "6. feature 11 (0.025204)\n",
      "7. feature 107 (0.022295)\n",
      "8. feature 74 (0.020907)\n",
      "9. feature 95 (0.020635)\n",
      "10. feature 80 (0.018854)\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import std, argsort\n",
    "\n",
    "importances = random_forest._clf.feature_importances_\n",
    "std = std([tree.feature_importances_ for tree in random_forest._clf.estimators_],\n",
    "             axis=0)\n",
    "indices = argsort(importances)[::-1]\n",
    "\n",
    "num_features = len(random_forest._clf.feature_importances_)\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(num_features), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(num_features), indices)\n",
    "plt.xlim([-1, num_features])\n",
    "plt.show()\n",
    "\n",
    "for f in range(10):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "print('...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scikit Learn metrics: Confusion matrix, Classification report, F1 score, Log loss\n",
    "\n",
    "http://scikit-learn.org/stable/modules/model_evaluation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def classification_report(predict):\n",
    "    predictions, labels = zip(*predict)\n",
    "    return metrics.classification_report(labels, [p.max() for p in predictions])\n",
    "\n",
    "def confusion_matrix(predict, print_layout=False):\n",
    "    predictions, labels = zip(*predict)\n",
    "    if print_layout is True:\n",
    "        print('Layout\\n[[tn   fp]\\n [fn   tp]]\\n')\n",
    "    return metrics.confusion_matrix(labels, [p.max() for p in predictions])\n",
    "    \n",
    "def log_loss(predict):\n",
    "    predictions, labels = zip(*predict)\n",
    "    return metrics.log_loss(labels, [p.prob('pos') for p in predictions])\n",
    "\n",
    "def roc_auc_score(predict):\n",
    "    predictions, labels = zip(*predict)\n",
    "    # need to convert labels to binary classification of 0 or 1\n",
    "    return metrics.roc_auc_score([1 if l == 'pos' else 0 for l in labels], [p.prob('pos') for p in predictions], average='weighted')\n",
    "\n",
    "def precision_recall_curve(predict):\n",
    "    predictions, labels = zip(*predict)\n",
    "    return metrics.precision_recall_curve(labels, [p.prob('pos') for p in predictions], pos_label='pos')\n",
    "\n",
    "def average_precision_score(predict):\n",
    "    predictions, labels = zip(*predict)\n",
    "    return metrics.average_precision_score([1 if l == 'pos' else 0 for l in labels], [p.prob('pos') for p in predictions])\n",
    "\n",
    "def roc_curve(predict):\n",
    "    predictions, labels = zip(*predict)\n",
    "    return metrics.roc_curve(labels, [p.prob('pos') for p in predictions], pos_label='pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.91      0.76      0.83       480\n",
      "        pos       0.78      0.91      0.84       437\n",
      "\n",
      "avg / total       0.85      0.84      0.83       917\n",
      "\n",
      "\n",
      "Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.90      0.77      0.83       480\n",
      "        pos       0.78      0.91      0.84       437\n",
      "\n",
      "avg / total       0.85      0.84      0.84       917\n",
      "\n",
      "\n",
      "LinearSVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.91      0.77      0.83       480\n",
      "        pos       0.78      0.91      0.84       437\n",
      "\n",
      "avg / total       0.85      0.84      0.84       917\n",
      "\n",
      "\n",
      "Random Forest\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.87      0.78      0.82       480\n",
      "        pos       0.78      0.87      0.83       437\n",
      "\n",
      "avg / total       0.83      0.82      0.82       917\n",
      "\n",
      "\n",
      "Voting\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.91      0.78      0.84       480\n",
      "        pos       0.79      0.91      0.84       437\n",
      "\n",
      "avg / total       0.85      0.84      0.84       917\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('SGD')\n",
    "print(classification_report(sgd_predict))\n",
    "print()\n",
    "print('Logistic Regression')\n",
    "print(classification_report(lr_predict))\n",
    "print()\n",
    "print('LinearSVC')\n",
    "print(classification_report(linear_svc_predict))\n",
    "print()\n",
    "print('Random Forest')\n",
    "print(classification_report(rf_predict))\n",
    "print()\n",
    "print('Voting')\n",
    "print(classification_report(voting_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layout\n",
      "[[tn   fp]\n",
      " [fn   tp]]\n",
      "\n",
      "SGD\n",
      "[[367 113]\n",
      " [ 38 399]]\n",
      "\n",
      "Logistic Regression\n",
      "[[370 110]\n",
      " [ 39 398]]\n",
      "\n",
      "LinearSVC\n",
      "[[368 112]\n",
      " [ 38 399]]\n",
      "\n",
      "Random Forest\n",
      "[[373 107]\n",
      " [ 55 382]]\n",
      "\n",
      "Voting\n",
      "[[372 108]\n",
      " [ 39 398]]\n"
     ]
    }
   ],
   "source": [
    "print('Layout\\n[[tn   fp]\\n [fn   tp]]\\n')\n",
    "\n",
    "print('SGD')\n",
    "print(confusion_matrix(sgd_predict))\n",
    "print()\n",
    "print('Logistic Regression')\n",
    "print(confusion_matrix(lr_predict))\n",
    "print()\n",
    "print('LinearSVC')\n",
    "print(confusion_matrix(linear_svc_predict))\n",
    "print()\n",
    "print('Random Forest')\n",
    "print(confusion_matrix(rf_predict))\n",
    "print()\n",
    "print('Voting')\n",
    "print(confusion_matrix(voting_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lower the better for `log_loss`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD: 0.3602928516558063\n",
      "Logistic Regression: 0.3576544666544726\n",
      "LinearSVC: 0.3587913221723783\n",
      "Random Forest: 0.4359850133656391\n",
      "Voting: 0.3440107245294793\n"
     ]
    }
   ],
   "source": [
    "print(f'SGD: {log_loss(sgd_predict)}')\n",
    "print(f'Logistic Regression: {log_loss(lr_predict)}')\n",
    "print(f'LinearSVC: {log_loss(linear_svc_predict)}')\n",
    "print(f'Random Forest: {log_loss(rf_predict)}')\n",
    "print(f'Voting: {log_loss(voting_predict)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The higher the better for `roc_auc_score`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD: 0.9245518688024409\n",
      "Logistic Regression: 0.9241228070175438\n",
      "LinearSVC: 0.9257389397406559\n",
      "Random Forest: 0.9106097444698704\n",
      "Voting: 0.9316266209000762\n"
     ]
    }
   ],
   "source": [
    "print(f'SGD: {roc_auc_score(sgd_predict)}')\n",
    "print(f'Logistic Regression: {roc_auc_score(lr_predict)}')\n",
    "print(f'LinearSVC: {roc_auc_score(linear_svc_predict)}')\n",
    "print(f'Random Forest: {roc_auc_score(rf_predict)}')\n",
    "print(f'Voting: {roc_auc_score(voting_predict)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance on sample tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy: [('neg', 0.0), ('neg', 0.0), ('neg', 0.0), ('neg', 0.0), ('pos', 1.0), ('neg', 0.0), ('pos', 1.0)]\n",
      "LogisticRegression: [('pos', 0.60220072135890279), ('pos', 0.75878324826920851), ('pos', 0.93270240152896788), ('pos', 0.75878324826920851), ('pos', 0.8530923605698969), ('pos', 0.75878324826920851), ('pos', 0.82376415483360399)]\n",
      "LinearSVC: [('pos', 0.60233895420655026), ('pos', 0.73213333246004275), ('pos', 0.90283543907030761), ('pos', 0.73213333246004275), ('pos', 0.83325600977805936), ('pos', 0.73213333246004275), ('pos', 0.7794113091492908)]\n",
      "SGD: [('pos', 0.60132912998097465), ('pos', 0.77418619826365875), ('pos', 0.94996303448610486), ('pos', 0.77418619826365875), ('pos', 0.89203930157365752), ('pos', 0.77418619826365875), ('pos', 0.84201090043390248)]\n",
      "Random Forest: [('pos', 0.55593879812347236), ('pos', 0.83079704227680562), ('pos', 0.79803072527460683), ('pos', 0.83079704227680562), ('pos', 0.90433333333333321), ('pos', 0.83079704227680562), ('pos', 0.83484852349181149)]\n",
      "\n",
      "Voting: [('pos', 0.59216916320077562), ('pos', 0.7707636615053115), ('pos', 0.89277136800947865), ('pos', 0.7707636615053115), ('pos', 0.84442041330171114), ('pos', 0.7707636615053115), ('pos', 0.81586444597441155)]\n"
     ]
    }
   ],
   "source": [
    "sample_tasks = [\"Mow lawn\", \"Mow the lawn\", \"Buy new shoes\", \"Feed the dog\", \"Send report to Kyle\", \"Send the report to Kyle\", \"Peel the potatoes\"]\n",
    "features = [featurize(nlp(task)) for task in sample_tasks]\n",
    "\n",
    "tasks_dummy = [(l, p.prob('pos')*1.0) for l, p in zip(dummy.classify_many(features), dummy.prob_classify_many(features))]\n",
    "tasks_logistic = [(l, p.prob('pos')) for l,p in zip(logistic_regression.classify_many(features), logistic_regression.prob_classify_many(features))]\n",
    "tasks_linear_svc = [(l, p.prob('pos')) for l,p in zip(linear_svc.classify_many(features), linear_svc.prob_classify_many(features))]\n",
    "tasks_sgd = [(l, p.prob('pos')) for l,p in zip(sgd.classify_many(features), sgd.prob_classify_many(features))]\n",
    "tasks_rf = [(l, p.prob('pos')) for l,p in zip(random_forest.classify_many(features), random_forest.prob_classify_many(features))]\n",
    "tasks_voting = [(l, p.prob('pos')) for l,p in zip(voting.classify_many(features), voting.prob_classify_many(features))]\n",
    "\n",
    "print(f'Dummy: {tasks_dummy}')\n",
    "print(f'LogisticRegression: {tasks_logistic}')\n",
    "print(f'LinearSVC: {tasks_linear_svc}')\n",
    "print(f'SGD: {tasks_sgd}')\n",
    "print(f'Random Forest: {tasks_rf}')\n",
    "print()\n",
    "print(f'Voting: {tasks_voting}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Voting Model: Curves\n",
    "\n",
    "Use the precision-recall curve and ROC to adjust the `pos`/`neg` threshold.\n",
    "\n",
    "The threshold defaults to 0.5, meaning a probability >0.5 of being `pos` will result in a `pos` classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nLNf/B2lS+6Z0PjsZAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x185aa9a6ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "precision, recall, prc_thresholds = precision_recall_curve(voting_predict)\n",
    "average_precision = average_precision_score(voting_predict)\n",
    "\n",
    "plt.figure()\n",
    "plt.step(recall, precision, color='b', alpha=0.2,\n",
    "         where='post')\n",
    "plt.fill_between(recall, precision, step='post', alpha=0.2,\n",
    "                 color='b')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('2-class Precision-Recall curve: AUC={0:0.2f}'.format(\n",
    "          average_precision))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find a `Threshold` before `Precision` drops below 0.8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Threshold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>0.878307</td>\n",
       "      <td>0.759725</td>\n",
       "      <td>0.597657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>0.877984</td>\n",
       "      <td>0.757437</td>\n",
       "      <td>0.605357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>0.877660</td>\n",
       "      <td>0.755149</td>\n",
       "      <td>0.618050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.755149</td>\n",
       "      <td>0.627285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>0.879679</td>\n",
       "      <td>0.752860</td>\n",
       "      <td>0.628034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>0.879357</td>\n",
       "      <td>0.750572</td>\n",
       "      <td>0.628471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>0.881720</td>\n",
       "      <td>0.750572</td>\n",
       "      <td>0.629713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>0.881402</td>\n",
       "      <td>0.748284</td>\n",
       "      <td>0.630727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>0.883784</td>\n",
       "      <td>0.748284</td>\n",
       "      <td>0.631524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>0.883469</td>\n",
       "      <td>0.745995</td>\n",
       "      <td>0.638043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>0.883152</td>\n",
       "      <td>0.743707</td>\n",
       "      <td>0.644309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>0.885559</td>\n",
       "      <td>0.743707</td>\n",
       "      <td>0.645518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>0.887978</td>\n",
       "      <td>0.743707</td>\n",
       "      <td>0.647074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>0.890411</td>\n",
       "      <td>0.743707</td>\n",
       "      <td>0.654082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>0.890110</td>\n",
       "      <td>0.741419</td>\n",
       "      <td>0.657239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>0.889807</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.659784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>0.892265</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.663040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>0.891967</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.663592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>0.894444</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.663780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>0.896936</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.668944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>0.899441</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.672862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>0.901961</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.673533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>0.901685</td>\n",
       "      <td>0.734554</td>\n",
       "      <td>0.674247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>0.904225</td>\n",
       "      <td>0.734554</td>\n",
       "      <td>0.679483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>0.906516</td>\n",
       "      <td>0.732265</td>\n",
       "      <td>0.680552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.732265</td>\n",
       "      <td>0.680617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>0.908832</td>\n",
       "      <td>0.729977</td>\n",
       "      <td>0.684624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>0.911429</td>\n",
       "      <td>0.729977</td>\n",
       "      <td>0.688237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>0.914040</td>\n",
       "      <td>0.729977</td>\n",
       "      <td>0.689967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>0.913793</td>\n",
       "      <td>0.727689</td>\n",
       "      <td>0.691510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>0.916427</td>\n",
       "      <td>0.727689</td>\n",
       "      <td>0.699777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>0.916185</td>\n",
       "      <td>0.725400</td>\n",
       "      <td>0.703873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>0.915942</td>\n",
       "      <td>0.723112</td>\n",
       "      <td>0.704949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>0.915698</td>\n",
       "      <td>0.720824</td>\n",
       "      <td>0.705841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>0.918367</td>\n",
       "      <td>0.720824</td>\n",
       "      <td>0.709599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>0.921053</td>\n",
       "      <td>0.720824</td>\n",
       "      <td>0.725225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>0.920821</td>\n",
       "      <td>0.718535</td>\n",
       "      <td>0.727586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>0.920588</td>\n",
       "      <td>0.716247</td>\n",
       "      <td>0.730840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>0.920354</td>\n",
       "      <td>0.713959</td>\n",
       "      <td>0.731372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>0.920118</td>\n",
       "      <td>0.711670</td>\n",
       "      <td>0.735150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>0.922849</td>\n",
       "      <td>0.711670</td>\n",
       "      <td>0.736923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>0.925595</td>\n",
       "      <td>0.711670</td>\n",
       "      <td>0.740635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>0.928358</td>\n",
       "      <td>0.711670</td>\n",
       "      <td>0.741134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>0.928144</td>\n",
       "      <td>0.709382</td>\n",
       "      <td>0.743213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>0.927928</td>\n",
       "      <td>0.707094</td>\n",
       "      <td>0.744555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.704805</td>\n",
       "      <td>0.745869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>0.930514</td>\n",
       "      <td>0.704805</td>\n",
       "      <td>0.749438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.704805</td>\n",
       "      <td>0.749972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>0.933131</td>\n",
       "      <td>0.702517</td>\n",
       "      <td>0.750222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>0.932927</td>\n",
       "      <td>0.700229</td>\n",
       "      <td>0.751701</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Precision    Recall  Threshold\n",
       "322   0.878307  0.759725   0.597657\n",
       "323   0.877984  0.757437   0.605357\n",
       "324   0.877660  0.755149   0.618050\n",
       "325   0.880000  0.755149   0.627285\n",
       "326   0.879679  0.752860   0.628034\n",
       "327   0.879357  0.750572   0.628471\n",
       "328   0.881720  0.750572   0.629713\n",
       "329   0.881402  0.748284   0.630727\n",
       "330   0.883784  0.748284   0.631524\n",
       "331   0.883469  0.745995   0.638043\n",
       "332   0.883152  0.743707   0.644309\n",
       "333   0.885559  0.743707   0.645518\n",
       "334   0.887978  0.743707   0.647074\n",
       "335   0.890411  0.743707   0.654082\n",
       "336   0.890110  0.741419   0.657239\n",
       "337   0.889807  0.739130   0.659784\n",
       "338   0.892265  0.739130   0.663040\n",
       "339   0.891967  0.736842   0.663592\n",
       "340   0.894444  0.736842   0.663780\n",
       "341   0.896936  0.736842   0.668944\n",
       "342   0.899441  0.736842   0.672862\n",
       "343   0.901961  0.736842   0.673533\n",
       "344   0.901685  0.734554   0.674247\n",
       "345   0.904225  0.734554   0.679483\n",
       "346   0.906516  0.732265   0.680552\n",
       "347   0.909091  0.732265   0.680617\n",
       "348   0.908832  0.729977   0.684624\n",
       "349   0.911429  0.729977   0.688237\n",
       "350   0.914040  0.729977   0.689967\n",
       "351   0.913793  0.727689   0.691510\n",
       "352   0.916427  0.727689   0.699777\n",
       "353   0.916185  0.725400   0.703873\n",
       "354   0.915942  0.723112   0.704949\n",
       "355   0.915698  0.720824   0.705841\n",
       "356   0.918367  0.720824   0.709599\n",
       "357   0.921053  0.720824   0.725225\n",
       "358   0.920821  0.718535   0.727586\n",
       "359   0.920588  0.716247   0.730840\n",
       "360   0.920354  0.713959   0.731372\n",
       "361   0.920118  0.711670   0.735150\n",
       "362   0.922849  0.711670   0.736923\n",
       "363   0.925595  0.711670   0.740635\n",
       "364   0.928358  0.711670   0.741134\n",
       "365   0.928144  0.709382   0.743213\n",
       "366   0.927928  0.707094   0.744555\n",
       "367   0.927711  0.704805   0.745869\n",
       "368   0.930514  0.704805   0.749438\n",
       "369   0.933333  0.704805   0.749972\n",
       "370   0.933131  0.702517   0.750222\n",
       "371   0.932927  0.700229   0.751701"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "\n",
    "prt_df = DataFrame.from_records(list(zip(precision, recall, prc_thresholds)), columns = ['Precision', 'Recall', 'Threshold'])\n",
    "prt_df.loc[(prt_df.Recall > 0.7) & (prt_df.Precision > 0.87)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pick a `Threshold` of 0.64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x185aa140c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fpr, tpr, roc_thresholds = roc_curve(voting_predict)\n",
    "area = roc_auc_score(voting_predict)\n",
    "\n",
    "plt.figure()\n",
    "plt.step(fpr, tpr, color='b', alpha=0.2,\n",
    "         where='post')\n",
    "plt.fill_between(fpr, tpr, step='post', alpha=0.2,\n",
    "                 color='b')\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('Receiving Operating Characteristic: area={0:0.2f}'.format(\n",
    "          area))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find a `Threshold` around where `True Positive Rate` quickly goes above 0.8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FPR</th>\n",
       "      <th>TPR</th>\n",
       "      <th>Threshold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.045833</td>\n",
       "      <td>0.704805</td>\n",
       "      <td>0.749972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.704805</td>\n",
       "      <td>0.745869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.711670</td>\n",
       "      <td>0.741134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.056250</td>\n",
       "      <td>0.711670</td>\n",
       "      <td>0.735150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.056250</td>\n",
       "      <td>0.720824</td>\n",
       "      <td>0.725225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.060417</td>\n",
       "      <td>0.720824</td>\n",
       "      <td>0.705841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.060417</td>\n",
       "      <td>0.727689</td>\n",
       "      <td>0.699777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.727689</td>\n",
       "      <td>0.691510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.729977</td>\n",
       "      <td>0.689967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.729977</td>\n",
       "      <td>0.684624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.732265</td>\n",
       "      <td>0.680617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.068750</td>\n",
       "      <td>0.732265</td>\n",
       "      <td>0.680552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.070833</td>\n",
       "      <td>0.734554</td>\n",
       "      <td>0.679483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.072917</td>\n",
       "      <td>0.734554</td>\n",
       "      <td>0.674247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.072917</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.673533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.081250</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.663592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.081250</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.663040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.659784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.743707</td>\n",
       "      <td>0.654082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.089583</td>\n",
       "      <td>0.743707</td>\n",
       "      <td>0.644309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.089583</td>\n",
       "      <td>0.748284</td>\n",
       "      <td>0.631524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.091667</td>\n",
       "      <td>0.748284</td>\n",
       "      <td>0.630727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.091667</td>\n",
       "      <td>0.750572</td>\n",
       "      <td>0.629713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.750572</td>\n",
       "      <td>0.628471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.755149</td>\n",
       "      <td>0.627285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.095833</td>\n",
       "      <td>0.755149</td>\n",
       "      <td>0.618050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.095833</td>\n",
       "      <td>0.759725</td>\n",
       "      <td>0.597657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.202083</td>\n",
       "      <td>0.878719</td>\n",
       "      <td>0.592169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.204167</td>\n",
       "      <td>0.878719</td>\n",
       "      <td>0.586585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.204167</td>\n",
       "      <td>0.881007</td>\n",
       "      <td>0.582470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.206250</td>\n",
       "      <td>0.881007</td>\n",
       "      <td>0.580896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.206250</td>\n",
       "      <td>0.885584</td>\n",
       "      <td>0.573711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.210417</td>\n",
       "      <td>0.885584</td>\n",
       "      <td>0.560152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.210417</td>\n",
       "      <td>0.890160</td>\n",
       "      <td>0.555927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.212500</td>\n",
       "      <td>0.890160</td>\n",
       "      <td>0.552813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.212500</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.549205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.214583</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.547179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.214583</td>\n",
       "      <td>0.903890</td>\n",
       "      <td>0.526918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.218750</td>\n",
       "      <td>0.903890</td>\n",
       "      <td>0.522051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.218750</td>\n",
       "      <td>0.906178</td>\n",
       "      <td>0.515995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         FPR       TPR  Threshold\n",
       "58  0.045833  0.704805   0.749972\n",
       "59  0.050000  0.704805   0.745869\n",
       "60  0.050000  0.711670   0.741134\n",
       "61  0.056250  0.711670   0.735150\n",
       "62  0.056250  0.720824   0.725225\n",
       "63  0.060417  0.720824   0.705841\n",
       "64  0.060417  0.727689   0.699777\n",
       "65  0.062500  0.727689   0.691510\n",
       "66  0.062500  0.729977   0.689967\n",
       "67  0.066667  0.729977   0.684624\n",
       "68  0.066667  0.732265   0.680617\n",
       "69  0.068750  0.732265   0.680552\n",
       "70  0.070833  0.734554   0.679483\n",
       "71  0.072917  0.734554   0.674247\n",
       "72  0.072917  0.736842   0.673533\n",
       "73  0.081250  0.736842   0.663592\n",
       "74  0.081250  0.739130   0.663040\n",
       "75  0.083333  0.739130   0.659784\n",
       "76  0.083333  0.743707   0.654082\n",
       "77  0.089583  0.743707   0.644309\n",
       "78  0.089583  0.748284   0.631524\n",
       "79  0.091667  0.748284   0.630727\n",
       "80  0.091667  0.750572   0.629713\n",
       "81  0.093750  0.750572   0.628471\n",
       "82  0.093750  0.755149   0.627285\n",
       "83  0.095833  0.755149   0.618050\n",
       "84  0.095833  0.759725   0.597657\n",
       "85  0.202083  0.878719   0.592169\n",
       "86  0.204167  0.878719   0.586585\n",
       "87  0.204167  0.881007   0.582470\n",
       "88  0.206250  0.881007   0.580896\n",
       "89  0.206250  0.885584   0.573711\n",
       "90  0.210417  0.885584   0.560152\n",
       "91  0.210417  0.890160   0.555927\n",
       "92  0.212500  0.890160   0.552813\n",
       "93  0.212500  0.894737   0.549205\n",
       "94  0.214583  0.894737   0.547179\n",
       "95  0.214583  0.903890   0.526918\n",
       "96  0.218750  0.903890   0.522051\n",
       "97  0.218750  0.906178   0.515995"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_df = DataFrame.from_records(list(zip(fpr, tpr, roc_thresholds)), columns = ['FPR', 'TPR', 'Threshold'])\n",
    "roc_df.loc[(roc_df.FPR < 0.22) & (roc_df.TPR > 0.7)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, a `Threshold` of ~0.596368 might be best for minimizing FPR and maximizing TPR (?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Might be a bad idea to actualy adjust `Threshold`, a form of overfitting on the test set: https://stackoverflow.com/questions/32627926/scikit-changing-the-threshold-to-create-multiple-confusion-matrixes (although using ROC to do this might be ok? https://stackoverflow.com/a/35300649)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps and Improvements\n",
    "\n",
    "1. Training set may be too specific/not relevant enough (recipe instructions for positive dataset, recipe descriptions+short movie reviews for negative dataset)\n",
    "2. Throwing features into a blender - need to understand value of each\n",
    "    - What feature \"classes\" tend to perform the best/worst?\n",
    "    - [PCA](http://jotterbach.github.io/2016/03/24/Principal_Component_Analysis/): Reducing dimensionality using most informative feature information\n",
    "3. Phrase vectorizations of all 0s - how problematic is this?\n",
    "4. Varying feature vector lengths - does this matter?\n",
    "5. Voting - POS taggers\n",
    "    - [SciKit Learn: Ensembles](http://scikit-learn.org/stable/modules/ensemble.html)\n",
    "    - [Kaggle Ensembling Guide](https://mlwave.com/kaggle-ensembling-guide/)\n",
    "6. Combining verb phrases\n",
    "7. Cross validation, grid search\n",
    "8. Look at examples from different quadrants of the confusion matrix - is there something we can learn?\n",
    "    - Same idea with the classification report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Things abandoned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I needed a library that supports dependency parsing, which NLTK does not... so I thought I'd add the [Stanford CoreNLP](https://stanfordnlp.github.io/CoreNLP/) toolkit and [its associated software](https://nlp.stanford.edu/software/) to NLTK. However, there are many conflicting instructions for installing the Java-based project, depending on NLTK version used. By the time I figured this out, the installation had become a time sink. So I abandoned this effort in favor of Spacy.io.\n",
    "\n",
    "I might return this way if I want to improve results/implement a voter system between the various linguistic and classification methods later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = [s for l in lines for s in sent_tokenize(l)] # punkt\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagged_sentences = []\n",
    "for s in sentences:\n",
    "    words = word_tokenize(s)\n",
    "    tagged = nltk.pos_tag(words) # averaged_perceptron_tagger\n",
    "    tagged_sentences.append(tagged)\n",
    "print(tagged_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: POS accuracy\n",
    "\n",
    "`Run down to the shop, will you, Peter` is parsed unexpectedly by `nltk.pos_tag`:\n",
    "> `[('Run', 'NNP'), ('down', 'RB'), ('to', 'TO'), ('the', 'DT'), ('shop', 'NN'), (',', ','), ('will', 'MD'), ('you', 'PRP'), (',', ','), ('Peter', 'NNP')]`\n",
    "\n",
    "`Run` is tagged as a `NNP (proper noun, singular)`\n",
    "\n",
    "I expected an output more like what the [Stanford Parser](http://nlp.stanford.edu:8080/parser/) provides:\n",
    "> `Run/VBG down/RP to/TO the/DT shop/NN ,/, will/MD you/PRP ,/, Peter/NNP`\n",
    "\n",
    "`Run` is tagged as a `VGB (verb, gerund/present participle)` - still not quite the `VB` I want, but at least it's a `V*`\n",
    "\n",
    "_MEANWHILE..._\n",
    "\n",
    "`nltk.pos_tag` did better with:\n",
    "> `[('Do', 'VB'), ('not', 'RB'), ('clean', 'VB'), ('soot', 'NN'), ('off', 'IN'), ('the', 'DT'), ('window', 'NN')]`\n",
    "\n",
    "Compared to [Stanford CoreNLP](http://nlp.stanford.edu:8080/corenlp/process) (note that this is different than what [Stanford Parser](http://nlp.stanford.edu:8080/parser/) outputs):\n",
    "> `(ROOT (S (VP (VB Do) (NP (RB not) (JJ clean) (NN soot)) (PP (IN off) (NP (DT the) (NN window))))))`\n",
    "\n",
    "Concern: _clean_ as `VB (verb, base form)` vs `JJ (adjective)` \n",
    "\n",
    "**IMPROVE** POS taggers should vote: nltk.pos_tag (averaged_perceptron_tagger), Stanford Parser, CoreNLP, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note what Spacy POS tagger did with `Run down to the shop, will you Peter`:\n",
    "\n",
    "`Run/VB down/RP to/IN the shop/NN ,/, will/MD you/PRP ,/, Peter/NNP`\n",
    "\n",
    "    where `Run` is the `VB` I expected from POS tagging (compared to `nltk.pos_tag` result of `NNP`). Also note that Spacy collapses `the shop` into a single unit, which should be helpful during featurization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Featurization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "featuresets = []\n",
    "for ts in tagged_sentences:\n",
    "    s_features = defaultdict(int)\n",
    "    for idx, tup in enumerate(ts):\n",
    "        #print(tup)\n",
    "        pos = tup[1]\n",
    "        # FeatureName.VERB\n",
    "        is_verb = re.match(r'VB.?', pos) is not None\n",
    "        print(tup, is_verb)\n",
    "        if is_verb:\n",
    "            s_features[FeatureName.VERB] += 1\n",
    "            # FOLLOWING_POS\n",
    "            next_idx = idx + 1;\n",
    "            if next_idx < len(ts):\n",
    "                s_features[f'{FeatureName.FOLLOWING}_{ts[next_idx][1]}'] += 1\n",
    "            # VERB_MODIFIER\n",
    "            # VERB_MODIFYING\n",
    "        else:\n",
    "            s_features[FeatureName.VERB] = 0\n",
    "    featuresets.append(dict(s_features))\n",
    "\n",
    "print()\n",
    "print(featuresets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Stanford NLP](https://nlp.stanford.edu/software/)\n",
    "Setup guide used: https://stackoverflow.com/a/34112695"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get dependency parser, NER, POS tagger\n",
    "!wget https://nlp.stanford.edu/software/stanford-parser-full-2017-06-09.zip\n",
    "!wget https://nlp.stanford.edu/software/stanford-ner-2017-06-09.zip\n",
    "!wget https://nlp.stanford.edu/software/stanford-postagger-full-2017-06-09.zip\n",
    "!unzip stanford-parser-full-2017-06-09.zip\n",
    "!unzip stanford-ner-2017-06-09.zip\n",
    "!unzip stanford-postagger-full-2017-06-09.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.parse.stanford import StanfordParser\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "from nltk.parse.stanford import StanfordNeuralDependencyParser\n",
    "from nltk.tag.stanford import StanfordPOSTagger, StanfordNERTagger\n",
    "from nltk.tokenize.stanford import StanfordTokenizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
